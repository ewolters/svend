{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate Reasoning Specialist (500M)\n",
        "\n",
        "This notebook evaluates the **Reasoning Specialist** model for its role in the Svend ensemble:\n",
        "\n",
        "**Role in Ensemble:**\n",
        "- Step-by-step reasoning chains\n",
        "- Mathematical problem solving\n",
        "- Tool calling (SymPy, Z3, Python sandbox)\n",
        "- Long context reasoning (8K tokens)\n",
        "- Gets verified by the Verifier model\n",
        "\n",
        "**What We Test:**\n",
        "1. Math reasoning (GSM8K-style)\n",
        "2. Multi-step logic\n",
        "3. Tool call formatting\n",
        "4. Chain-of-thought quality\n",
        "5. Answer extraction reliability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Drive to load checkpoint\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo\n",
        "!rm -rf /content/svend\n",
        "!git clone https://github.com/ewolters/svend.git /content/svend\n",
        "%cd /content/svend\n",
        "!git log -1 --oneline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('src')]\n",
        "for mod in modules_to_remove:\n",
        "    del sys.modules[mod]\n",
        "if '/content/svend' not in sys.path:\n",
        "    sys.path.insert(0, '/content/svend')\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "from src.models.config import get_config\n",
        "from src.models.transformer import ReasoningTransformer\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to your trained checkpoint\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/svend-checkpoints/base-reasoner/final.pt\"\n",
        "# Or use a step checkpoint:\n",
        "# CHECKPOINT_PATH = \"/content/drive/MyDrive/svend-checkpoints/base-reasoner/step_001000.pt\"\n",
        "\n",
        "MODEL_SIZE = \"500m\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model_config = get_config(MODEL_SIZE)\n",
        "model_config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "model = ReasoningTransformer(model_config)\n",
        "\n",
        "# Load weights\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.cuda()\n",
        "model.eval()\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Loaded model: {params/1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation helper\n",
        "@torch.no_grad()\n",
        "def generate(prompt, max_tokens=256, temperature=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=temperature > 0,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluation Benchmarks\n",
        "\n",
        "Testing capabilities critical for the Reasoning Specialist role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation problems designed for reasoning specialist\n",
        "\n",
        "EVAL_PROBLEMS = {\n",
        "    \"math_basic\": [\n",
        "        {\"q\": \"What is 15% of 200?\", \"a\": \"30\"},\n",
        "        {\"q\": \"If a train travels at 60 mph for 2.5 hours, how far does it go?\", \"a\": \"150\"},\n",
        "        {\"q\": \"What is 7 * 8 + 12 / 4?\", \"a\": \"59\"},\n",
        "        {\"q\": \"A shirt costs $25. With a 20% discount, what is the final price?\", \"a\": \"20\"},\n",
        "        {\"q\": \"If 3x + 7 = 22, what is x?\", \"a\": \"5\"},\n",
        "    ],\n",
        "    \n",
        "    \"math_multistep\": [\n",
        "        {\n",
        "            \"q\": \"A snail climbs 3 meters up a wall during the day but slides down 2 meters at night. If the wall is 10 meters tall, how many days does it take to reach the top?\",\n",
        "            \"a\": \"8\"\n",
        "        },\n",
        "        {\n",
        "            \"q\": \"If 5 machines can produce 5 widgets in 5 minutes, how many minutes would it take 100 machines to produce 100 widgets?\",\n",
        "            \"a\": \"5\"\n",
        "        },\n",
        "        {\n",
        "            \"q\": \"A store has a 'buy 2 get 1 free' deal on $10 items. How much do you pay for 7 items?\",\n",
        "            \"a\": \"50\"\n",
        "        },\n",
        "        {\n",
        "            \"q\": \"John is twice as old as Mary. In 5 years, the sum of their ages will be 40. How old is Mary now?\",\n",
        "            \"a\": \"10\"\n",
        "        },\n",
        "    ],\n",
        "    \n",
        "    \"logic\": [\n",
        "        {\n",
        "            \"q\": \"If all cats are mammals, and some mammals are pets, can we conclude that some cats are pets? Answer yes or no.\",\n",
        "            \"a\": \"no\"\n",
        "        },\n",
        "        {\n",
        "            \"q\": \"Alice is taller than Bob. Bob is taller than Charlie. Is Alice taller than Charlie? Answer yes or no.\",\n",
        "            \"a\": \"yes\"\n",
        "        },\n",
        "        {\n",
        "            \"q\": \"If it rains, the ground is wet. The ground is wet. Did it rain? Answer yes, no, or cannot determine.\",\n",
        "            \"a\": \"cannot determine\"\n",
        "        },\n",
        "    ],\n",
        "    \n",
        "    \"tool_format\": [\n",
        "        {\n",
        "            \"q\": \"Use the calculator tool to compute the derivative of x^3 + 2x. Format: <tool_call>calculator: [expression]</tool_call>\",\n",
        "            \"check\": \"tool_call\",\n",
        "            \"expected_contains\": [\"tool_call\", \"derivative\", \"3*x\"]\n",
        "        },\n",
        "        {\n",
        "            \"q\": \"Use the python tool to calculate 17 factorial. Format: <tool_call>python: [code]</tool_call>\",\n",
        "            \"check\": \"tool_call\",\n",
        "            \"expected_contains\": [\"tool_call\", \"factorial\", \"17\"]\n",
        "        },\n",
        "    ],\n",
        "    \n",
        "    \"chain_of_thought\": [\n",
        "        {\n",
        "            \"q\": \"Think step by step: A farmer has 17 sheep. All but 9 die. How many are left?\",\n",
        "            \"a\": \"9\",\n",
        "            \"check_cot\": True  # Check that reasoning appears before answer\n",
        "        },\n",
        "        {\n",
        "            \"q\": \"Think step by step: I have a 3-gallon jug and a 5-gallon jug. How can I measure exactly 4 gallons?\",\n",
        "            \"check_cot\": True,\n",
        "            \"expected_steps\": 3  # Should have multiple reasoning steps\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"Total problems: {sum(len(v) for v in EVAL_PROBLEMS.values())}\")\n",
        "for cat, probs in EVAL_PROBLEMS.items():\n",
        "    print(f\"  {cat}: {len(probs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_number(text):\n",
        "    \"\"\"Extract the last number from text.\"\"\"\n",
        "    # Look for explicit answer patterns first\n",
        "    patterns = [\n",
        "        r\"(?:answer|result)\\s*(?:is|=|:)?\\s*(-?\\d+(?:\\.\\d+)?)\",\n",
        "        r\"=\\s*(-?\\d+(?:\\.\\d+)?)\\s*$\",\n",
        "        r\"####\\s*(-?\\d+(?:\\.\\d+)?)\",\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    \n",
        "    # Fall back to last number\n",
        "    numbers = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
        "    return numbers[-1] if numbers else None\n",
        "\n",
        "def check_answer(response, expected):\n",
        "    \"\"\"Check if response contains the expected answer.\"\"\"\n",
        "    response_lower = response.lower()\n",
        "    expected_lower = expected.lower()\n",
        "    \n",
        "    # Direct containment\n",
        "    if expected_lower in response_lower:\n",
        "        return True\n",
        "    \n",
        "    # Numeric comparison\n",
        "    try:\n",
        "        extracted = extract_number(response)\n",
        "        if extracted:\n",
        "            return abs(float(extracted) - float(expected)) < 0.01\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return False\n",
        "\n",
        "def check_tool_format(response, expected_contains):\n",
        "    \"\"\"Check if response has proper tool call format.\"\"\"\n",
        "    response_lower = response.lower()\n",
        "    return all(term.lower() in response_lower for term in expected_contains)\n",
        "\n",
        "def count_reasoning_steps(response):\n",
        "    \"\"\"Count reasoning steps in response.\"\"\"\n",
        "    # Look for numbered steps, bullet points, or 'step' mentions\n",
        "    step_patterns = [\n",
        "        r\"step\\s*\\d\",\n",
        "        r\"^\\d+\\.\",\n",
        "        r\"^-\\s\",\n",
        "        r\"first|second|third|then|next|finally\",\n",
        "    ]\n",
        "    count = 0\n",
        "    for pattern in step_patterns:\n",
        "        count += len(re.findall(pattern, response, re.IGNORECASE | re.MULTILINE))\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "results = {}\n",
        "\n",
        "for category, problems in EVAL_PROBLEMS.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Category: {category}\")\n",
        "    print('='*50)\n",
        "    \n",
        "    category_results = []\n",
        "    \n",
        "    for i, prob in enumerate(problems):\n",
        "        prompt = f\"Question: {prob['q']}\\n\\nAnswer:\"\n",
        "        response = generate(prompt, max_tokens=300, temperature=0.3)\n",
        "        \n",
        "        # Extract just the generated part\n",
        "        generated = response[len(prompt):].strip()\n",
        "        \n",
        "        # Evaluate based on problem type\n",
        "        result = {\"question\": prob['q'], \"response\": generated}\n",
        "        \n",
        "        if prob.get('check') == 'tool_call':\n",
        "            result['correct'] = check_tool_format(generated, prob['expected_contains'])\n",
        "        elif 'a' in prob:\n",
        "            result['correct'] = check_answer(generated, prob['a'])\n",
        "            result['expected'] = prob['a']\n",
        "        \n",
        "        if prob.get('check_cot'):\n",
        "            result['reasoning_steps'] = count_reasoning_steps(generated)\n",
        "            result['has_reasoning'] = result['reasoning_steps'] >= 2\n",
        "        \n",
        "        category_results.append(result)\n",
        "        \n",
        "        # Print result\n",
        "        status = \"PASS\" if result.get('correct', result.get('has_reasoning', False)) else \"FAIL\"\n",
        "        print(f\"\\n[{status}] Q{i+1}: {prob['q'][:60]}...\")\n",
        "        print(f\"  Response: {generated[:100]}...\" if len(generated) > 100 else f\"  Response: {generated}\")\n",
        "    \n",
        "    results[category] = category_results\n",
        "    \n",
        "    # Category summary\n",
        "    if category_results:\n",
        "        if 'correct' in category_results[0]:\n",
        "            acc = sum(r['correct'] for r in category_results) / len(category_results)\n",
        "            print(f\"\\n{category} Accuracy: {acc:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION SUMMARY - Reasoning Specialist (500M)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_correct = 0\n",
        "total_problems = 0\n",
        "\n",
        "for category, cat_results in results.items():\n",
        "    if cat_results and 'correct' in cat_results[0]:\n",
        "        correct = sum(r['correct'] for r in cat_results)\n",
        "        total = len(cat_results)\n",
        "        total_correct += correct\n",
        "        total_problems += total\n",
        "        print(f\"{category:20s}: {correct}/{total} ({correct/total:.1%})\")\n",
        "    elif cat_results and 'has_reasoning' in cat_results[0]:\n",
        "        has_cot = sum(r.get('has_reasoning', False) for r in cat_results)\n",
        "        avg_steps = sum(r.get('reasoning_steps', 0) for r in cat_results) / len(cat_results)\n",
        "        print(f\"{category:20s}: {has_cot}/{len(cat_results)} with CoT, avg {avg_steps:.1f} steps\")\n",
        "\n",
        "if total_problems > 0:\n",
        "    print(f\"\\n{'Overall':20s}: {total_correct}/{total_problems} ({total_correct/total_problems:.1%})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ROLE FITNESS ASSESSMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Assess fitness for ensemble role\n",
        "assessments = []\n",
        "\n",
        "# Math capability\n",
        "math_results = results.get('math_basic', []) + results.get('math_multistep', [])\n",
        "if math_results:\n",
        "    math_acc = sum(r['correct'] for r in math_results) / len(math_results)\n",
        "    if math_acc >= 0.7:\n",
        "        assessments.append((\"Math Reasoning\", \"GOOD\", f\"{math_acc:.1%}\"))\n",
        "    elif math_acc >= 0.4:\n",
        "        assessments.append((\"Math Reasoning\", \"NEEDS WORK\", f\"{math_acc:.1%}\"))\n",
        "    else:\n",
        "        assessments.append((\"Math Reasoning\", \"POOR\", f\"{math_acc:.1%}\"))\n",
        "\n",
        "# Logic capability\n",
        "logic_results = results.get('logic', [])\n",
        "if logic_results:\n",
        "    logic_acc = sum(r['correct'] for r in logic_results) / len(logic_results)\n",
        "    if logic_acc >= 0.6:\n",
        "        assessments.append((\"Logic\", \"GOOD\", f\"{logic_acc:.1%}\"))\n",
        "    else:\n",
        "        assessments.append((\"Logic\", \"NEEDS WORK\", f\"{logic_acc:.1%}\"))\n",
        "\n",
        "# Tool format capability\n",
        "tool_results = results.get('tool_format', [])\n",
        "if tool_results:\n",
        "    tool_acc = sum(r['correct'] for r in tool_results) / len(tool_results)\n",
        "    if tool_acc >= 0.5:\n",
        "        assessments.append((\"Tool Formatting\", \"GOOD\", f\"{tool_acc:.1%}\"))\n",
        "    else:\n",
        "        assessments.append((\"Tool Formatting\", \"NEEDS TRAINING\", f\"{tool_acc:.1%}\"))\n",
        "\n",
        "# Chain of thought\n",
        "cot_results = results.get('chain_of_thought', [])\n",
        "if cot_results:\n",
        "    has_cot = sum(r.get('has_reasoning', False) for r in cot_results) / len(cot_results)\n",
        "    if has_cot >= 0.7:\n",
        "        assessments.append((\"Chain-of-Thought\", \"GOOD\", f\"{has_cot:.1%}\"))\n",
        "    else:\n",
        "        assessments.append((\"Chain-of-Thought\", \"NEEDS WORK\", f\"{has_cot:.1%}\"))\n",
        "\n",
        "for skill, status, score in assessments:\n",
        "    print(f\"{skill:20s}: {status:12s} ({score})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"RECOMMENDATIONS FOR TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "recommendations = []\n",
        "\n",
        "# Based on assessments\n",
        "for skill, status, score in assessments:\n",
        "    if status == \"NEEDS WORK\" or status == \"POOR\":\n",
        "        if \"Math\" in skill:\n",
        "            recommendations.append(\"- Add more GSM8K/MATH training data\")\n",
        "            recommendations.append(\"- Include step-by-step solutions in training\")\n",
        "        elif \"Logic\" in skill:\n",
        "            recommendations.append(\"- Add logic puzzle datasets (e.g., LogiQA)\")\n",
        "            recommendations.append(\"- Train on syllogistic reasoning examples\")\n",
        "        elif \"Tool\" in skill:\n",
        "            recommendations.append(\"- Add synthetic tool-calling examples\")\n",
        "            recommendations.append(\"- Fine-tune on <tool_call> format specifically\")\n",
        "        elif \"Chain\" in skill:\n",
        "            recommendations.append(\"- Use 'Let's think step by step' prompting in training\")\n",
        "            recommendations.append(\"- Train on CoT-annotated datasets\")\n",
        "    elif status == \"NEEDS TRAINING\":\n",
        "        if \"Tool\" in skill:\n",
        "            recommendations.append(\"- CRITICAL: Model needs tool-call format training\")\n",
        "            recommendations.append(\"- Generate synthetic tool-calling dataset\")\n",
        "\n",
        "if not recommendations:\n",
        "    print(\"Model looks ready for ensemble integration!\")\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Train the Verifier model to validate this model's outputs\")\n",
        "    print(\"2. Train the Router to direct queries here\")\n",
        "    print(\"3. Integration testing with full ensemble\")\n",
        "else:\n",
        "    for rec in recommendations:\n",
        "        print(rec)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation results\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "eval_output = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"model\": MODEL_SIZE,\n",
        "    \"checkpoint\": CHECKPOINT_PATH,\n",
        "    \"results\": results,\n",
        "    \"assessments\": assessments,\n",
        "}\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/svend-checkpoints/base-reasoner/eval_results.json\"\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(eval_output, f, indent=2, default=str)\n",
        "\n",
        "print(f\"Results saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Interactive Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test custom prompts\n",
        "test_prompt = \"Question: A bat and a ball cost $1.10. The bat costs $1.00 more than the ball. How much does the ball cost?\\n\\nAnswer:\"\n",
        "\n",
        "response = generate(test_prompt, max_tokens=200, temperature=0.3)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tool calling format\n",
        "tool_prompt = \"\"\"You have access to these tools:\n",
        "- calculator: For math computations\n",
        "- python: For running code\n",
        "\n",
        "Question: What is the integral of x^2 dx?\n",
        "\n",
        "Use <tool_call>tool_name: arguments</tool_call> format if needed.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "response = generate(tool_prompt, max_tokens=200, temperature=0.3)\n",
        "print(response)"
      ]
    }
  ]
}
