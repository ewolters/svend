{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Svend Reasoning Specialist Training\n\n**Complete pipeline: Generate tool traces â†’ Train reasoning specialist**\n\nThis notebook:\n1. Generates synthetic tool-calling training data (10K+ examples)\n2. Fine-tunes the language model for reasoning and tool use\n\n**Setup:**\n1. Add `ANTHROPIC_API_KEY` to Colab Secrets (key icon in left sidebar)\n2. Run all cells\n\n**Prerequisites:** \n- Trained language model checkpoint in Drive\n- ~$30 API budget for 10K tool traces"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup and API Key"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mount Drive and load API key\nfrom google.colab import drive, userdata\ndrive.mount('/content/drive')\n\nimport os\ntry:\n    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n    os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n    print(f'Loaded ANTHROPIC_API_KEY (length: {len(ANTHROPIC_API_KEY)})')\nexcept Exception as e:\n    ANTHROPIC_API_KEY = None\n    print(f'ERROR: {e}')\n    print('Add ANTHROPIC_API_KEY to Colab Secrets')\n\n!pip install -q anthropic transformers accelerate datasets wandb\n\nos.chdir('/content')\n!rm -rf svend\n!git clone https://github.com/ewolters/svend.git\nos.chdir('/content/svend')\n\nimport sys\nsys.path.insert(0, '/content/svend')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Config { display-mode: \"form\" }\n#@markdown ### Paths\nDRIVE_BASE = '/content/drive/MyDrive/svend-checkpoints'  #@param {type:\"string\"}\nBASE_CHECKPOINT = f'{DRIVE_BASE}/language-model/checkpoint-200000.pt'  #@param {type:\"string\"}\nOUTPUT_DIR = f'{DRIVE_BASE}/reasoning-specialist'  #@param {type:\"string\"}\n\n#@markdown ### Data Generation\nGENERATE_TOOL_TRACES = True  #@param {type:\"boolean\"}\nNUM_TOOL_TRACES = 10000  #@param {type:\"integer\"}\nTOOL_TRACES_FILE = f'{DRIVE_BASE}/data/tool_traces.jsonl'\n\n#@markdown ### Training\nMAX_STEPS = 20000  #@param {type:\"integer\"}\nBATCH_SIZE = 4  #@param {type:\"integer\"}\nGRAD_ACCUM = 8  #@param {type:\"integer\"}\nLEARNING_RATE = 5e-5  #@param {type:\"number\"}\nMAX_SEQ_LENGTH = 2048  #@param {type:\"integer\"}\n\n#@markdown ### Logging\nUSE_WANDB = False  #@param {type:\"boolean\"}\n\nfrom pathlib import Path\nPath(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\nPath(TOOL_TRACES_FILE).parent.mkdir(parents=True, exist_ok=True)\n\nprint(f'Base checkpoint: {BASE_CHECKPOINT}')\nprint(f'Output dir: {OUTPUT_DIR}')\nprint(f'Generate {NUM_TOOL_TRACES} tool traces: {GENERATE_TOOL_TRACES}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 2. Generate Tool Traces (if enabled)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nfrom pathlib import Path\n\nif GENERATE_TOOL_TRACES:\n    # Check if we already have enough traces\n    existing_count = 0\n    if Path(TOOL_TRACES_FILE).exists():\n        with open(TOOL_TRACES_FILE) as f:\n            existing_count = sum(1 for _ in f)\n        print(f'Found {existing_count} existing traces')\n    \n    if existing_count < NUM_TOOL_TRACES:\n        print(f'\\\\n=== Generating {NUM_TOOL_TRACES} tool traces ===')\n        print('This may take 30-60 minutes and cost ~$30 in API calls')\n        print('='*60)\n        \n        env = os.environ.copy()\n        \n        process = subprocess.Popen(\n            ['python', 'scripts/generate_tool_data.py', \n             '--num-examples', str(NUM_TOOL_TRACES),\n             '--output', TOOL_TRACES_FILE],\n            cwd='/content/svend',\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1\n        )\n        \n        for line in process.stdout:\n            print(line, end='')\n        \n        process.wait()\n        print('='*60)\n        print(f'Exit code: {process.returncode}')\n    else:\n        print(f'Skipping generation - already have {existing_count} traces')\nelse:\n    print('Tool trace generation disabled')\n\n# Count final traces\nif Path(TOOL_TRACES_FILE).exists():\n    with open(TOOL_TRACES_FILE) as f:\n        final_count = sum(1 for _ in f)\n    print(f'\\\\nTool traces available: {final_count}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Setup PyTorch"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nimport json\nimport math\nimport random\nfrom datetime import datetime\nfrom tqdm.auto import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'Device: {device}')\nif device.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name()}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n\ndtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\nprint(f'Training dtype: {dtype}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Load Base Language Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.models.config import TransformerConfig\nfrom src.models.transformer import ReasoningTransformer\n\nprint(f'Loading base model from: {BASE_CHECKPOINT}')\ncheckpoint = torch.load(BASE_CHECKPOINT, map_location='cpu', weights_only=False)\n\n# Get config from checkpoint\nconfig_dict = checkpoint.get('config', {})\nif isinstance(config_dict, dict):\n    config_dict = {k: v for k, v in config_dict.items() if k != 'head_dim'}\n    base_config = TransformerConfig(**config_dict)\nelse:\n    base_config = config_dict\n\nprint(f'Base model: {base_config.hidden_size}h x {base_config.num_hidden_layers}L')\nprint(f'Parameters: ~{base_config.num_parameters() / 1e6:.0f}M')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create model and load weights\nmodel = ReasoningTransformer(base_config)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel = model.to(device)\n\nparams = sum(p.numel() for p in model.parameters())\nprint(f'Model loaded: {params/1e6:.1f}M parameters')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup tokenizer with special reasoning tokens\ntokenizer = AutoTokenizer.from_pretrained(checkpoint.get('tokenizer_name', 'gpt2'))\ntokenizer.pad_token = tokenizer.eos_token\n\n# Add special tokens for tool calling\nspecial_tokens = {\n    'additional_special_tokens': [\n        '<|think|>', '<|/think|>',\n        '<|step|>', '<|/step|>',\n        '<|tool_call|>', '<|/tool_call|>',\n        '<|tool_result|>', '<|/tool_result|>',\n        '<|answer|>', '<|/answer|>',\n        '<|verify|>', '<|/verify|>',\n    ]\n}\n\nnum_added = tokenizer.add_special_tokens(special_tokens)\nprint(f'Added {num_added} special tokens')\nprint(f'Vocab size: {len(tokenizer)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Resize embeddings if vocab changed\nif len(tokenizer) > model.embed_tokens.weight.shape[0]:\n    old_embeddings = model.embed_tokens.weight.data\n    new_vocab_size = len(tokenizer)\n    \n    new_embeddings = nn.Embedding(new_vocab_size, base_config.hidden_size)\n    new_embeddings.weight.data[:old_embeddings.shape[0]] = old_embeddings\n    new_embeddings.weight.data[old_embeddings.shape[0]:] = old_embeddings.mean(dim=0)\n    model.embed_tokens = new_embeddings.to(device)\n    \n    if hasattr(model, 'lm_head') and model.lm_head is not None:\n        old_lm_head = model.lm_head.weight.data\n        new_lm_head = nn.Linear(base_config.hidden_size, new_vocab_size, bias=False)\n        new_lm_head.weight.data[:old_lm_head.shape[0]] = old_lm_head\n        new_lm_head.weight.data[old_lm_head.shape[0]:] = old_lm_head.mean(dim=0)\n        model.lm_head = new_lm_head.to(device)\n    \n    print(f'Resized embeddings: {old_embeddings.shape[0]} -> {new_vocab_size}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Load Training Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def format_tool_trace(data):\n    \"\"\"Format a tool trace into training text.\"\"\"\n    text = f\"Question: {data['question']}\\n\\n<|think|>\\n\"\n    \n    for step in data.get('reasoning', []):\n        specialist = step.get('specialist', 'reasoning')\n        content = step.get('content', '')\n        \n        text += f\"[{specialist.upper()}] {content}\\n\"\n        \n        if 'tool_call' in step:\n            tc = step['tool_call']\n            text += f\"<|tool_call|>{tc.get('name', '')}({json.dumps(tc.get('args', {}))})<|/tool_call|>\\n\"\n        \n        if 'tool_result' in step:\n            text += f\"<|tool_result|>{step['tool_result']}<|/tool_result|>\\n\"\n    \n    text += \"<|/think|>\\n\\n\"\n    text += f\"<|answer|>{data.get('answer', '')}<|/answer|>\"\n    \n    return text\n\n# Load tool traces\nall_examples = []\n\nif Path(TOOL_TRACES_FILE).exists():\n    with open(TOOL_TRACES_FILE, 'r') as f:\n        for line in f:\n            if line.strip():\n                data = json.loads(line)\n                text = format_tool_trace(data)\n                all_examples.append({'text': text, 'source': 'tool_trace'})\n    print(f'Loaded {len(all_examples)} tool traces')\nelse:\n    print(f'WARNING: No tool traces found at {TOOL_TRACES_FILE}')\n\n# Also load GSM8K for math reasoning\ntry:\n    from datasets import load_dataset\n    gsm8k = load_dataset('openai/gsm8k', 'main', split='train')\n    for ex in gsm8k:\n        question = ex['question']\n        answer = ex['answer']\n        if '####' in answer:\n            reasoning, final = answer.rsplit('####', 1)\n        else:\n            reasoning, final = answer, ''\n        \n        text = f\"Question: {question}\\n\\n<|think|>\\n{reasoning.strip()}\\n<|/think|>\\n\\n<|answer|>{final.strip()}<|/answer|>\"\n        all_examples.append({'text': text, 'source': 'gsm8k'})\n    print(f'Loaded {len(gsm8k)} GSM8K examples')\nexcept Exception as e:\n    print(f'GSM8K load failed: {e}')\n\nprint(f'\\nTotal training examples: {len(all_examples)}')\nrandom.seed(42)\nrandom.shuffle(all_examples)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preview examples\nprint('Sample tool trace:')\nprint('='*60)\nfor ex in all_examples[:2]:\n    print(f\"[{ex['source']}]\")\n    print(ex['text'][:500])\n    print('...\\n' + '-'*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Create Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ReasoningDataset(Dataset):\n    def __init__(self, examples, tokenizer, max_length=2048):\n        self.examples = examples\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        text = self.examples[idx]['text']\n        \n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n        )\n        \n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        labels = input_ids.clone()\n        labels[attention_mask == 0] = -100\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels,\n        }\n\ndataset = ReasoningDataset(all_examples, tokenizer, MAX_SEQ_LENGTH)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n\nprint(f'Dataset: {len(dataset)} examples')\nprint(f'Batches: {len(dataloader)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Training Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import get_cosine_schedule_with_warmup\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=MAX_STEPS)\nscaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None\n\nprint(f'Optimizer: AdamW, LR={LEARNING_RATE}')\nprint(f'Scheduler: Cosine with 500 warmup steps')\nprint(f'Max steps: {MAX_STEPS}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USE_WANDB:\n    import wandb\n    wandb.init(project='svend-reasoning', name=f'reasoning-{datetime.now().strftime(\"%m%d-%H%M\")}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def save_checkpoint(step, loss):\n    path = f'{OUTPUT_DIR}/checkpoint-{step}.pt'\n    torch.save({\n        'step': step,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'config': config_dict,\n        'tokenizer_name': 'gpt2',\n        'special_tokens': special_tokens,\n        'loss': loss,\n    }, path)\n    print(f'Saved: {path}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Training Loop"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('='*60)\nprint('TRAINING REASONING SPECIALIST')\nprint('='*60)\n\nmodel.train()\nstep = 0\nepoch = 0\ntotal_loss = 0\nbest_loss = float('inf')\n\nprogress = tqdm(total=MAX_STEPS, desc='Training')\n\ntry:\n    while step < MAX_STEPS:\n        epoch += 1\n        \n        for batch in dataloader:\n            if step >= MAX_STEPS:\n                break\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            if scaler:\n                with torch.amp.autocast('cuda', dtype=dtype):\n                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                    loss = outputs['loss'] / GRAD_ACCUM\n                scaler.scale(loss).backward()\n            else:\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs['loss'] / GRAD_ACCUM\n                loss.backward()\n            \n            total_loss += loss.item() * GRAD_ACCUM\n            \n            if (step + 1) % GRAD_ACCUM == 0:\n                if scaler:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n            \n            step += 1\n            progress.update(1)\n            \n            if step % 50 == 0:\n                avg_loss = total_loss / 50\n                progress.set_postfix({\n                    'loss': f'{avg_loss:.4f}',\n                    'ppl': f'{math.exp(min(avg_loss, 10)):.1f}',\n                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n                })\n                \n                if USE_WANDB:\n                    wandb.log({'loss': avg_loss, 'lr': scheduler.get_last_lr()[0], 'step': step})\n                \n                total_loss = 0\n            \n            if step % 2000 == 0:\n                save_checkpoint(step, avg_loss)\n                if avg_loss < best_loss:\n                    best_loss = avg_loss\n\nexcept KeyboardInterrupt:\n    print('\\\\nInterrupted!')\n    save_checkpoint(step, total_loss / max(1, step % 50))\n\nprogress.close()\nprint(f'\\\\nTraining complete at step {step}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Save Final Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "final_path = f'{OUTPUT_DIR}/final-reasoning-specialist.pt'\n\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'config': config_dict,\n    'tokenizer_name': 'gpt2',\n    'special_tokens': special_tokens,\n    'training_steps': step,\n    'base_model': BASE_CHECKPOINT,\n}, final_path)\n\nprint(f'Final model saved: {final_path}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Test Generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef test_reasoning(prompt):\n    model.eval()\n    full_prompt = f\"Question: {prompt}\\n\\n<|think|>\\n\"\n    input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n    \n    with torch.amp.autocast('cuda', dtype=dtype):\n        output = model.generate(\n            input_ids,\n            max_new_tokens=512,\n            temperature=0.3,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    return tokenizer.decode(output[0], skip_special_tokens=False)\n\n# Test problems\ntest_problems = [\n    \"What is the derivative of x^3 + 2x?\",\n    \"A store sells apples for $2 each. If I buy 5 apples, how much do I spend?\",\n    \"Find the area of a circle with radius 5.\",\n]\n\nprint('='*60)\nprint('TEST GENERATION')\nprint('='*60)\nfor p in test_problems:\n    print(f'\\n{p}')\n    print('-'*40)\n    response = test_reasoning(p)\n    print(response[:600])\n    if len(response) > 600:\n        print('...')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USE_WANDB:\n    wandb.finish()\n\nprint('\\n' + '='*60)\nprint('REASONING SPECIALIST TRAINING COMPLETE')\nprint('='*60)\nprint(f'Checkpoint: {final_path}')\nprint(f'Steps: {step:,}')\nprint(f'Best loss: {best_loss:.4f}')\nprint('='*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}