{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svend Reasoning Specialist Fine-Tuning\n",
    "\n",
    "**Stage 2 of 2-stage training pipeline**\n",
    "\n",
    "This notebook fine-tunes the pretrained language model for mathematical reasoning and chain-of-thought.\n",
    "\n",
    "**Prerequisites:** \n",
    "- Trained language model from `train_language_model.ipynb`\n",
    "- Checkpoint at `/content/drive/MyDrive/svend-checkpoints/language-model/final-language-model.pt`\n",
    "\n",
    "**Goal:** Learn step-by-step reasoning, math problem solving, and tool calling.\n",
    "\n",
    "**Output:** Reasoning specialist model ready for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repository\n!git clone https://github.com/ewolters/svend.git 2>/dev/null || echo \"Already cloned\"\n%cd svend"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/ewolters/svend.git reasoning-lab 2>/dev/null || echo \"Already cloned\"\n",
    "%cd reasoning-lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, get_cosine_schedule_with_warmup\nfrom datasets import load_dataset\nimport os\nimport math\nimport random\nfrom datetime import datetime\nfrom tqdm.auto import tqdm\n\nimport sys\nsys.path.insert(0, '/content/svend')\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/reasoning-lab')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Base model (output from train_language_model.ipynb)\n",
    "    \"base_model_path\": \"/content/drive/MyDrive/svend-checkpoints/language-model/final-language-model.pt\",\n",
    "    \n",
    "    # Training\n",
    "    \"max_steps\": 20_000,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation\": 8,  # Effective batch = 32\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"learning_rate\": 5e-5,  # Lower than pretraining\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \n",
    "    # Checkpointing\n",
    "    \"checkpoint_dir\": \"/content/drive/MyDrive/svend-checkpoints/reasoning-specialist\",\n",
    "    \"save_every\": 2000,\n",
    "    \"eval_every\": 500,\n",
    "    \n",
    "    # Logging\n",
    "    \"use_wandb\": True,\n",
    "    \"wandb_project\": \"svend-reasoning\",\n",
    "    \"experiment_name\": f\"reasoning-500m-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    "    \n",
    "    # Resume\n",
    "    \"resume_from\": None,\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Base model: {CONFIG['base_model_path']}\")\n",
    "print(f\"  Max steps: {CONFIG['max_steps']:,}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Effective batch: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.config import TransformerConfig\n",
    "from src.models.transformer import ReasoningTransformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained language model checkpoint\n",
    "print(f\"Loading base model from: {CONFIG['base_model_path']}\")\n",
    "checkpoint = torch.load(CONFIG[\"base_model_path\"], map_location=device, weights_only=False)\n",
    "\n",
    "# Get the config from checkpoint\n",
    "base_config = TransformerConfig(**checkpoint[\"config\"])\n",
    "print(f\"\\nBase model config:\")\n",
    "print(f\"  Name: {base_config.name}\")\n",
    "print(f\"  Parameters: {base_config.num_parameters() / 1e6:.0f}M\")\n",
    "print(f\"  Hidden size: {base_config.hidden_size}\")\n",
    "print(f\"  Layers: {base_config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reasoning specialist config (same architecture, enable tool calling)\n",
    "reasoning_config = TransformerConfig(\n",
    "    name=\"svend-reasoning-500m\",\n",
    "    model_type=\"reasoning\",\n",
    "    vocab_size=base_config.vocab_size,\n",
    "    hidden_size=base_config.hidden_size,\n",
    "    intermediate_size=base_config.intermediate_size,\n",
    "    num_hidden_layers=base_config.num_hidden_layers,\n",
    "    num_attention_heads=base_config.num_attention_heads,\n",
    "    num_key_value_heads=base_config.num_key_value_heads,\n",
    "    max_position_embeddings=base_config.max_position_embeddings,\n",
    "    hidden_act=base_config.hidden_act,\n",
    "    tool_calling=True,\n",
    "    num_tool_tokens=64,\n",
    ")\n",
    "\n",
    "print(f\"Reasoning specialist config:\")\n",
    "print(f\"  Name: {reasoning_config.name}\")\n",
    "print(f\"  Tool calling: {reasoning_config.tool_calling}\")\n",
    "print(f\"  Context length: {reasoning_config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and load pretrained weights\n",
    "model = ReasoningTransformer(reasoning_config)\n",
    "\n",
    "# Load weights from language model\n",
    "pretrained_state = checkpoint[\"model_state_dict\"]\n",
    "model_state = model.state_dict()\n",
    "\n",
    "loaded_keys = []\n",
    "skipped_keys = []\n",
    "\n",
    "for key in pretrained_state:\n",
    "    if key in model_state and pretrained_state[key].shape == model_state[key].shape:\n",
    "        model_state[key] = pretrained_state[key]\n",
    "        loaded_keys.append(key)\n",
    "    else:\n",
    "        skipped_keys.append(key)\n",
    "\n",
    "model.load_state_dict(model_state)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Loaded {len(loaded_keys)} weight tensors from language model\")\n",
    "if skipped_keys:\n",
    "    print(f\"Skipped {len(skipped_keys)} tensors (shape mismatch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tokenizer with special reasoning tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint.get(\"tokenizer_name\", \"gpt2\"))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add special tokens for reasoning\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<|think|>\", \"<|/think|>\",\n",
    "        \"<|step|>\", \"<|/step|>\",\n",
    "        \"<|tool_call|>\", \"<|/tool_call|>\",\n",
    "        \"<|tool_result|>\", \"<|/tool_result|>\",\n",
    "        \"<|answer|>\", \"<|/answer|>\",\n",
    "        \"<|verify|>\", \"<|/verify|>\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "num_added = tokenizer.add_special_tokens(special_tokens)\n",
    "print(f\"Added {num_added} special tokens\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Resize embeddings if needed\n",
    "if len(tokenizer) > model.embed_tokens.weight.shape[0]:\n",
    "    old_embeddings = model.embed_tokens.weight.data\n",
    "    new_vocab_size = len(tokenizer)\n",
    "    \n",
    "    # Create new embedding layer\n",
    "    new_embeddings = nn.Embedding(new_vocab_size, reasoning_config.hidden_size)\n",
    "    new_embeddings.weight.data[:old_embeddings.shape[0]] = old_embeddings\n",
    "    # Initialize new tokens with mean of existing\n",
    "    new_embeddings.weight.data[old_embeddings.shape[0]:] = old_embeddings.mean(dim=0)\n",
    "    \n",
    "    model.embed_tokens = new_embeddings.to(device)\n",
    "    \n",
    "    # Update lm_head if not tied\n",
    "    if model.lm_head is not None:\n",
    "        old_lm_head = model.lm_head.weight.data\n",
    "        new_lm_head = nn.Linear(reasoning_config.hidden_size, new_vocab_size, bias=False)\n",
    "        new_lm_head.weight.data[:old_lm_head.shape[0]] = old_lm_head\n",
    "        new_lm_head.weight.data[old_lm_head.shape[0]:] = old_lm_head.mean(dim=0)\n",
    "        model.lm_head = new_lm_head.to(device)\n",
    "    \n",
    "    # Update config\n",
    "    reasoning_config.vocab_size = new_vocab_size\n",
    "    print(f\"Resized embeddings to {new_vocab_size}\")\n",
    "\n",
    "# Mixed precision dtype\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "print(f\"\\nModel ready on {device}, dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Reasoning Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_gsm8k(example):\n",
    "    \"\"\"Format GSM8K example with chain-of-thought.\"\"\"\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    \n",
    "    if \"####\" in answer:\n",
    "        reasoning, final = answer.rsplit(\"####\", 1)\n",
    "        reasoning = reasoning.strip()\n",
    "        final = final.strip()\n",
    "    else:\n",
    "        reasoning = answer\n",
    "        final = \"\"\n",
    "    \n",
    "    return {\"text\": f\"\"\"Question: {question}\n",
    "\n",
    "<|think|>\n",
    "{reasoning}\n",
    "<|/think|>\n",
    "\n",
    "<|answer|>{final}<|/answer|>\"\"\"}\n",
    "\n",
    "\n",
    "def format_math(example):\n",
    "    \"\"\"Format MATH dataset example.\"\"\"\n",
    "    problem = example.get(\"problem\", \"\")\n",
    "    solution = example.get(\"solution\", \"\")\n",
    "    \n",
    "    return {\"text\": f\"\"\"Problem: {problem}\n",
    "\n",
    "<|think|>\n",
    "{solution}\n",
    "<|/think|>\"\"\"}\n",
    "\n",
    "\n",
    "def format_arc(example):\n",
    "    \"\"\"Format ARC example.\"\"\"\n",
    "    question = example[\"question\"]\n",
    "    choices = example[\"choices\"]\n",
    "    answer_key = example[\"answerKey\"]\n",
    "    \n",
    "    choice_text = \"\\n\".join([\n",
    "        f\"{label}. {text}\" \n",
    "        for label, text in zip(choices[\"label\"], choices[\"text\"])\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        correct_idx = choices[\"label\"].index(answer_key)\n",
    "        correct_text = choices[\"text\"][correct_idx]\n",
    "    except ValueError:\n",
    "        correct_text = answer_key\n",
    "    \n",
    "    return {\"text\": f\"\"\"Question: {question}\n",
    "\n",
    "Choices:\n",
    "{choice_text}\n",
    "\n",
    "<|think|>\n",
    "Let me analyze each option.\n",
    "The correct answer is {answer_key}: {correct_text}\n",
    "<|/think|>\n",
    "\n",
    "<|answer|>{answer_key}<|/answer|>\"\"\"}\n",
    "\n",
    "\n",
    "def format_orca_math(example):\n",
    "    \"\"\"Format Orca Math example.\"\"\"\n",
    "    question = example.get(\"question\", \"\")\n",
    "    answer = example.get(\"answer\", \"\")\n",
    "    \n",
    "    return {\"text\": f\"\"\"Question: {question}\n",
    "\n",
    "<|think|>\n",
    "{answer}\n",
    "<|/think|>\"\"\"}\n",
    "\n",
    "\n",
    "print(\"Formatting functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and format all datasets\n",
    "all_examples = []\n",
    "\n",
    "# GSM8K\n",
    "print(\"Loading GSM8K...\")\n",
    "try:\n",
    "    gsm8k = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "    for ex in gsm8k:\n",
    "        formatted = format_gsm8k(ex)\n",
    "        all_examples.append({\"text\": formatted[\"text\"], \"source\": \"gsm8k\"})\n",
    "    print(f\"  GSM8K: {len(gsm8k)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"  GSM8K failed: {e}\")\n",
    "\n",
    "# MATH (competition_math)\n",
    "print(\"Loading MATH...\")\n",
    "try:\n",
    "    math_ds = load_dataset(\"lighteval/MATH\", \"all\", split=\"train\")\n",
    "    for ex in math_ds:\n",
    "        formatted = format_math(ex)\n",
    "        all_examples.append({\"text\": formatted[\"text\"], \"source\": \"math\"})\n",
    "    print(f\"  MATH: {len(math_ds)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"  MATH failed: {e}\")\n",
    "\n",
    "# ARC-Challenge\n",
    "print(\"Loading ARC...\")\n",
    "try:\n",
    "    arc = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"train\")\n",
    "    for ex in arc:\n",
    "        formatted = format_arc(ex)\n",
    "        all_examples.append({\"text\": formatted[\"text\"], \"source\": \"arc\"})\n",
    "    print(f\"  ARC: {len(arc)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"  ARC failed: {e}\")\n",
    "\n",
    "# Orca Math (sample 20k)\n",
    "print(\"Loading Orca Math...\")\n",
    "try:\n",
    "    orca = load_dataset(\"microsoft/orca-math-word-problems-200k\", split=\"train\")\n",
    "    orca = orca.shuffle(seed=42).select(range(min(20000, len(orca))))\n",
    "    for ex in orca:\n",
    "        formatted = format_orca_math(ex)\n",
    "        all_examples.append({\"text\": formatted[\"text\"], \"source\": \"orca_math\"})\n",
    "    print(f\"  Orca Math: {len(orca)} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"  Orca Math failed: {e}\")\n",
    "\n",
    "print(f\"\\nTotal examples: {len(all_examples)}\")\n",
    "\n",
    "# Count by source\n",
    "from collections import Counter\n",
    "source_counts = Counter(ex[\"source\"] for ex in all_examples)\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"  {source}: {count}\")\n",
    "\n",
    "# Shuffle\n",
    "random.seed(42)\n",
    "random.shuffle(all_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a few examples\n",
    "print(\"Sample examples:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(3, len(all_examples))):\n",
    "    ex = all_examples[i]\n",
    "    print(f\"\\n[{ex['source'].upper()}]\")\n",
    "    print(ex[\"text\"][:400])\n",
    "    if len(ex[\"text\"]) > 400:\n",
    "        print(\"...\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningDataset(Dataset):\n",
    "    \"\"\"Dataset for reasoning fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, tokenizer, max_length=2048):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx][\"text\"]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = ReasoningDataset(all_examples, tokenizer, CONFIG[\"max_seq_length\"])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} examples\")\n",
    "print(f\"Batches per epoch: {len(dataloader)}\")\n",
    "print(f\"Steps per epoch: {len(dataloader) // CONFIG['gradient_accumulation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    num_training_steps=CONFIG[\"max_steps\"],\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(dtype == torch.float16))\n",
    "\n",
    "print(\"Optimizer and scheduler configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"use_wandb\"]:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=CONFIG[\"wandb_project\"],\n",
    "        name=CONFIG[\"experiment_name\"],\n",
    "        config=CONFIG,\n",
    "    )\n",
    "    print(f\"WandB initialized: {CONFIG['experiment_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, step, loss, path):\n",
    "    torch.save({\n",
    "        \"step\": step,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"config\": reasoning_config.to_dict(),\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"special_tokens\": special_tokens,\n",
    "    }, path)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, scheduler):\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "    return ckpt[\"step\"], ckpt.get(\"loss\", 0)\n",
    "\n",
    "\n",
    "start_step = 0\n",
    "if CONFIG[\"resume_from\"]:\n",
    "    start_step, _ = load_checkpoint(CONFIG[\"resume_from\"], model, optimizer, scheduler)\n",
    "    print(f\"Resumed from step {start_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_reasoning(model, tokenizer, num_samples=5):\n",
    "    \"\"\"Generate sample outputs to check reasoning quality.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_problems = [\n",
    "        \"Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market for $2 per egg. How much does she make every day?\",\n",
    "        \"Question: A train travels at 60 mph for 2 hours, then at 80 mph for 1.5 hours. What is the total distance traveled?\",\n",
    "        \"Question: If x + 3 = 7, what is the value of x?\",\n",
    "        \"Question: A rectangle has a perimeter of 24 cm. If the length is twice the width, what are the dimensions?\",\n",
    "        \"Question: There are 5 red balls and 3 blue balls in a bag. What is the probability of drawing a red ball?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"REASONING EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for problem in test_problems[:num_samples]:\n",
    "        prompt = problem + \"\\n\\n<|think|>\\n\"\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.amp.autocast(\"cuda\", dtype=dtype):\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(response[:600])\n",
    "        if len(response) > 600:\n",
    "            print(\"...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING REASONING SPECIALIST FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base model: {CONFIG['base_model_path']}\")\n",
    "print(f\"Max steps: {CONFIG['max_steps']:,}\")\n",
    "print(f\"Dataset: {len(dataset)} examples\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "model.train()\n",
    "step = start_step\n",
    "epoch = 0\n",
    "total_loss = 0\n",
    "log_interval = 50\n",
    "\n",
    "progress = tqdm(total=CONFIG[\"max_steps\"], initial=start_step, desc=\"Training\")\n",
    "\n",
    "try:\n",
    "    while step < CONFIG[\"max_steps\"]:\n",
    "        epoch += 1\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            if step >= CONFIG[\"max_steps\"]:\n",
    "                break\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            with torch.amp.autocast(\"cuda\", dtype=dtype):\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs[\"loss\"] / CONFIG[\"gradient_accumulation\"]\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            total_loss += loss.item() * CONFIG[\"gradient_accumulation\"]\n",
    "            \n",
    "            if (step + 1) % CONFIG[\"gradient_accumulation\"] == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"max_grad_norm\"])\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            step += 1\n",
    "            progress.update(1)\n",
    "            \n",
    "            if step % log_interval == 0:\n",
    "                avg_loss = total_loss / log_interval\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                \n",
    "                progress.set_postfix({\n",
    "                    \"loss\": f\"{avg_loss:.4f}\",\n",
    "                    \"ppl\": f\"{math.exp(min(avg_loss, 10)):.1f}\",\n",
    "                    \"lr\": f\"{lr:.2e}\",\n",
    "                })\n",
    "                \n",
    "                if CONFIG[\"use_wandb\"]:\n",
    "                    wandb.log({\n",
    "                        \"train/loss\": avg_loss,\n",
    "                        \"train/perplexity\": math.exp(min(avg_loss, 10)),\n",
    "                        \"train/lr\": lr,\n",
    "                        \"train/step\": step,\n",
    "                        \"train/epoch\": epoch,\n",
    "                    })\n",
    "                \n",
    "                total_loss = 0\n",
    "            \n",
    "            if step % CONFIG[\"eval_every\"] == 0:\n",
    "                evaluate_reasoning(model, tokenizer, num_samples=3)\n",
    "            \n",
    "            if step % CONFIG[\"save_every\"] == 0:\n",
    "                path = os.path.join(CONFIG[\"checkpoint_dir\"], f\"checkpoint-{step}.pt\")\n",
    "                save_checkpoint(model, optimizer, scheduler, step, avg_loss, path)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrupted. Saving checkpoint...\")\n",
    "    path = os.path.join(CONFIG[\"checkpoint_dir\"], f\"checkpoint-{step}-interrupted.pt\")\n",
    "    save_checkpoint(model, optimizer, scheduler, step, total_loss / max(1, step % log_interval), path)\n",
    "\n",
    "progress.close()\n",
    "print(f\"\\nTraining finished at step {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = os.path.join(CONFIG[\"checkpoint_dir\"], \"final-reasoning-specialist.pt\")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": reasoning_config.to_dict(),\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"special_tokens\": special_tokens,\n",
    "    \"training_steps\": step,\n",
    "    \"base_model\": CONFIG[\"base_model_path\"],\n",
    "}, final_path)\n",
    "\n",
    "print(f\"Final model saved: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Evaluation:\")\n",
    "evaluate_reasoning(model, tokenizer, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG[\"use_wandb\"]:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REASONING SPECIALIST TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Checkpoint: {final_path}\")\n",
    "print(f\"Steps: {step:,}\")\n",
    "print(f\"Epochs: {epoch}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Deploy**: Use `scripts/run_server.py` to serve the model\n",
    "2. **Evaluate**: Run `scripts/run_unified_eval.py` for benchmarks\n",
    "3. **Train Verifier**: Optional critic model for self-verification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}