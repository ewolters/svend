{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Svend Ensemble Training on Colab\n",
        "\n",
        "Train the 4-model specialist ensemble:\n",
        "\n",
        "| Model | Size | Role |\n",
        "|-------|------|------|\n",
        "| **Router** | 125M | Intent classification, routes to specialists |\n",
        "| **Language** | 500M | Prompt interpretation, synthesis, output |\n",
        "| **Reasoning** | 500M | Math, logic, chain-of-thought, tools |\n",
        "| **Verifier** | 250M | Checks answers, catches errors |\n",
        "\n",
        "**Total: ~1.4B parameters across 4 specialist models**\n",
        "\n",
        "This is more efficient than a single 7B model and allows:\n",
        "- Faster inference (router picks which model runs)\n",
        "- Specialized training per domain\n",
        "- Mix-and-match during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    if gpu_memory > 30:\n",
        "        print(\"\\n[OK] Sufficient GPU memory for ensemble training!\")\n",
        "    else:\n",
        "        print(\"\\n[OK] Can train models sequentially\")\n",
        "else:\n",
        "    print(\"[X] No GPU - training will be very slow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch transformers datasets accelerate wandb sympy\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/YOUR_USERNAME/reasoning-lab.git\n",
        "%cd reasoning-lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create checkpoint directory\n",
        "import os\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/svend-ensemble'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from src.models import (\n",
        "    create_model,\n",
        "    create_router_config,\n",
        "    create_language_specialist_config,\n",
        "    create_reasoning_specialist_config,\n",
        "    create_verifier_specialist_config,\n",
        "    create_ensemble_config,\n",
        ")\n",
        "from src.data import create_tokenizer\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print ensemble summary\n",
        "ensemble = create_ensemble_config()\n",
        "ensemble.print_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "CONFIG = {\n",
        "    # Which models to train (set to False to skip)\n",
        "    \"train_router\": True,\n",
        "    \"train_language\": True,\n",
        "    \"train_reasoning\": True,\n",
        "    \"train_verifier\": True,\n",
        "    \n",
        "    # Data\n",
        "    \"num_samples\": 50000,  # Per model (None for full dataset)\n",
        "    \"max_seq_length\": 2048,\n",
        "    \n",
        "    # Training (per model)\n",
        "    \"epochs\": 3,\n",
        "    \"batch_size\": 8,\n",
        "    \"gradient_accumulation\": 4,\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \n",
        "    # Efficiency\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"bf16\": True,\n",
        "    \n",
        "    # Saving\n",
        "    \"save_steps\": 500,\n",
        "    \"output_dir\": CHECKPOINT_DIR,\n",
        "    \n",
        "    # Logging\n",
        "    \"use_wandb\": True,\n",
        "    \"wandb_project\": \"svend-ensemble\",\n",
        "}\n",
        "\n",
        "# Model-specific learning rates\n",
        "LEARNING_RATES = {\n",
        "    \"router\": 5e-4,      # Smaller model, can use higher LR\n",
        "    \"language\": 1e-4,\n",
        "    \"reasoning\": 1e-4,\n",
        "    \"verifier\": 3e-4,    # Smaller model\n",
        "}\n",
        "\n",
        "print(\"Configuration set!\")\n",
        "print(f\"Training: Router={CONFIG['train_router']}, Language={CONFIG['train_language']}, \"\n",
        "      f\"Reasoning={CONFIG['train_reasoning']}, Verifier={CONFIG['train_verifier']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer (shared across all models)\n",
        "tokenizer = create_tokenizer(\n",
        "    base_tokenizer=\"mistralai/Mistral-7B-v0.1\",\n",
        "    vocab_size=32000,\n",
        "    add_reasoning_tokens=True,\n",
        ")\n",
        "print(f\"Tokenizer vocabulary: {len(tokenizer)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data import DatasetConfig, create_combined_dataset, ReasoningDataset, create_dataloaders\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "data_config = DatasetConfig(max_seq_length=CONFIG[\"max_seq_length\"])\n",
        "\n",
        "try:\n",
        "    dataset = create_combined_dataset(data_config)\n",
        "    \n",
        "    if CONFIG[\"num_samples\"] and len(dataset) > CONFIG[\"num_samples\"]:\n",
        "        dataset = dataset.select(range(CONFIG[\"num_samples\"]))\n",
        "    \n",
        "    print(f\"Total examples: {len(dataset)}\")\n",
        "    \n",
        "    # Split\n",
        "    train_size = int(0.95 * len(dataset))\n",
        "    train_data = dataset.select(range(train_size))\n",
        "    val_data = dataset.select(range(train_size, len(dataset)))\n",
        "    \n",
        "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
        "    \n",
        "    train_dataset = ReasoningDataset(train_data, tokenizer, max_length=CONFIG[\"max_seq_length\"])\n",
        "    val_dataset = ReasoningDataset(val_data, tokenizer, max_length=CONFIG[\"max_seq_length\"])\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not load full dataset: {e}\")\n",
        "    print(\"Using synthetic data for testing...\")\n",
        "    # Fallback to synthetic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.training import TrainingConfig, Trainer\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def train_specialist(name, config_fn, train_dataset, val_dataset, tokenizer):\n",
        "    \"\"\"Train a single specialist model.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {name.upper()} Specialist\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Create model config\n",
        "    config = config_fn()\n",
        "    config.gradient_checkpointing = CONFIG[\"gradient_checkpointing\"]\n",
        "    \n",
        "    print(f\"Model: {config.name}\")\n",
        "    print(f\"Parameters: {config.num_parameters() / 1e6:.0f}M\")\n",
        "    \n",
        "    # Create model\n",
        "    model = create_model(config)\n",
        "    \n",
        "    # Resize embeddings if needed\n",
        "    if len(tokenizer) > config.vocab_size:\n",
        "        model.embed_tokens = torch.nn.Embedding(len(tokenizer), config.hidden_size)\n",
        "    \n",
        "    # Create dataloaders\n",
        "    dataloaders = create_dataloaders(\n",
        "        train_dataset, val_dataset,\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        num_workers=2,\n",
        "    )\n",
        "    \n",
        "    # Output directory\n",
        "    output_dir = os.path.join(CONFIG[\"output_dir\"], name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Training config\n",
        "    training_config = TrainingConfig(\n",
        "        num_epochs=CONFIG[\"epochs\"],\n",
        "        learning_rate=LEARNING_RATES[name],\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
        "        warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
        "        mixed_precision=True,\n",
        "        bf16=CONFIG[\"bf16\"],\n",
        "        gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
        "        output_dir=output_dir,\n",
        "        save_steps=CONFIG[\"save_steps\"],\n",
        "        use_wandb=CONFIG[\"use_wandb\"],\n",
        "        wandb_project=CONFIG[\"wandb_project\"],\n",
        "        wandb_run_name=f\"{name}-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
        "    )\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        config=training_config,\n",
        "        train_dataloader=dataloaders[\"train\"],\n",
        "        eval_dataloader=dataloaders.get(\"val\"),\n",
        "    )\n",
        "    \n",
        "    # Train!\n",
        "    print(f\"\\nStarting training...\")\n",
        "    results = trainer.train()\n",
        "    \n",
        "    # Save final model\n",
        "    final_path = os.path.join(output_dir, \"final\")\n",
        "    os.makedirs(final_path, exist_ok=True)\n",
        "    torch.save(model.state_dict(), os.path.join(final_path, \"model.pt\"))\n",
        "    config.save(os.path.join(final_path, \"config.json\"))\n",
        "    tokenizer.save_pretrained(final_path)\n",
        "    \n",
        "    print(f\"\\n[DONE] {name} saved to: {final_path}\")\n",
        "    \n",
        "    # Free memory\n",
        "    del model, trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Router (125M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"train_router\"]:\n",
        "    router_results = train_specialist(\n",
        "        \"router\",\n",
        "        create_router_config,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        tokenizer\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping router training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Language Specialist (500M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"train_language\"]:\n",
        "    language_results = train_specialist(\n",
        "        \"language\",\n",
        "        create_language_specialist_config,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        tokenizer\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping language training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Reasoning Specialist (500M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"train_reasoning\"]:\n",
        "    reasoning_results = train_specialist(\n",
        "        \"reasoning\",\n",
        "        create_reasoning_specialist_config,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        tokenizer\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping reasoning training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Verifier Specialist (250M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"train_verifier\"]:\n",
        "    verifier_results = train_specialist(\n",
        "        \"verifier\",\n",
        "        create_verifier_specialist_config,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        tokenizer\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping verifier training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Complete!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENSEMBLE TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nModels saved to: {CONFIG['output_dir']}\")\n",
        "print(\"\\nDirectory structure:\")\n",
        "!ls -la {CONFIG['output_dir']}\n",
        "\n",
        "print(\"\\n\\nNext steps:\")\n",
        "print(\"1. Download models from Google Drive\")\n",
        "print(\"2. Run inference locally with the ensemble\")\n",
        "print(\"3. Fine-tune individual specialists as needed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Inference (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test of the reasoning specialist\n",
        "import os\n",
        "\n",
        "reasoning_path = os.path.join(CONFIG['output_dir'], 'reasoning', 'final')\n",
        "\n",
        "if os.path.exists(reasoning_path):\n",
        "    print(\"Loading reasoning specialist for test...\")\n",
        "    \n",
        "    config = create_reasoning_specialist_config()\n",
        "    model = create_model(config)\n",
        "    model.load_state_dict(torch.load(os.path.join(reasoning_path, 'model.pt')))\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    \n",
        "    # Test prompts\n",
        "    prompts = [\n",
        "        \"What is 15 + 27?\",\n",
        "        \"Find the derivative of x^2 + 3x\",\n",
        "    ]\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "        \n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"Response: {response}\")\n",
        "else:\n",
        "    print(\"Reasoning model not found - skipping test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compress all models for download\n",
        "!tar -czvf svend-ensemble.tar.gz -C /content/drive/MyDrive svend-ensemble\n",
        "\n",
        "from google.colab import files\n",
        "files.download('svend-ensemble.tar.gz')"
      ]
    }
  ]
}
