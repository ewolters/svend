{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svend Language Model Pretraining\n",
    "\n",
    "**Stage 1 of 2-stage training pipeline**\n",
    "\n",
    "This notebook trains the base language model that will later be fine-tuned for reasoning.\n",
    "\n",
    "**Goal:** Learn language patterns, grammar, and world knowledge from diverse text data.\n",
    "\n",
    "**Output:** Base language model checkpoint to be used by `train_reasoning_specialist.ipynb`\n",
    "\n",
    "**Requirements:**\n",
    "- Colab Pro+ (A100 recommended)\n",
    "- Google Drive for checkpoint persistence\n",
    "- ~10-20 hours training time for 500M model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive first (for checkpoint persistence)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repository\n!git clone https://github.com/ewolters/svend.git 2>/dev/null || echo \"Already cloned\"\n%cd svend"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets accelerate wandb\n",
    "!pip install -q sentencepiece tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom transformers import AutoTokenizer, get_cosine_schedule_with_warmup\nfrom datasets import load_dataset\nimport os\nimport json\nimport math\nfrom pathlib import Path\nfrom datetime import datetime\nfrom tqdm.auto import tqdm\n\n# Add src to path\nimport sys\nsys.path.insert(0, '/content/svend')\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TRAINING CONFIGURATION\n# =============================================================================\n\n# =============================================\n# QUICK CONFIG - CHANGE THESE FOR EACH RUN\n# =============================================\n# Resume from your latest checkpoint (100K steps)\nRESUME_CHECKPOINT = \"/content/drive/MyDrive/svend-checkpoints/language-model/checkpoint-100000.pt\"\nADDITIONAL_STEPS = 50_000  # Steps to train THIS run (50K = ~30-45 min on A100)\n# =============================================\n\nCONFIG = {\n    # Model\n    \"model_size\": \"500m\",\n\n    # Data - diverse mix for language understanding\n    \"datasets\": [\n        {\"name\": \"openwebtext\", \"subset\": None, \"weight\": 0.35},\n        {\"name\": \"wikimedia/wikipedia\", \"subset\": \"20231101.en\", \"weight\": 0.25},\n        {\"name\": \"allenai/c4\", \"subset\": \"en\", \"weight\": 0.20},\n        {\"name\": \"HuggingFaceFW/fineweb\", \"subset\": \"sample-10BT\", \"weight\": 0.20},\n    ],\n    \n    # Training\n    \"max_steps\": ADDITIONAL_STEPS,  # Updated after loading checkpoint\n    \"batch_size\": 8,\n    \"gradient_accumulation\": 4,  # Effective batch = 32\n    \"max_seq_length\": 1024,\n    \"learning_rate\": 1e-4,  # Lower LR for continued training\n    \"weight_decay\": 0.1,\n    \"warmup_steps\": 500,  # Short warmup for resume\n    \"max_grad_norm\": 1.0,\n    \n    # Checkpointing\n    \"checkpoint_dir\": \"/content/drive/MyDrive/svend-checkpoints/language-model\",\n    \"save_every\": 10000,  # Save every 10K\n    \"eval_every\": None,\n    \n    # Logging\n    \"use_wandb\": True,\n    \"wandb_project\": \"svend-language\",\n    \"experiment_name\": f\"lm-500m-continued-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n    \n    # Resume\n    \"resume_from\": RESUME_CHECKPOINT,\n}\n\nos.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n\nprint(\"=\"*60)\nprint(\"SVEND LANGUAGE MODEL - CONTINUED TRAINING\")\nprint(\"=\"*60)\nprint(f\"Resume from: checkpoint-100000.pt\")\nprint(f\"Additional steps: {ADDITIONAL_STEPS:,}\")\nprint(f\"Target: 150K total steps\")\nprint(f\"Learning rate: {CONFIG['learning_rate']} (reduced for fine-tuning)\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.config import create_language_specialist_config\n",
    "from src.models.transformer import ReasoningTransformer\n",
    "\n",
    "# Create language model config\n",
    "model_config = create_language_specialist_config()\n",
    "\n",
    "# Adjust for language pretraining (no tool tokens needed yet)\n",
    "model_config.tool_calling = False\n",
    "model_config.num_tool_tokens = 0\n",
    "\n",
    "print(f\"Model: {model_config.name}\")\n",
    "print(f\"Parameters: {model_config.num_parameters() / 1e6:.0f}M\")\n",
    "print(f\"Hidden size: {model_config.hidden_size}\")\n",
    "print(f\"Layers: {model_config.num_hidden_layers}\")\n",
    "print(f\"Attention heads: {model_config.num_attention_heads}\")\n",
    "print(f\"Context length: {model_config.max_position_embeddings}\")\n",
    "\n",
    "memory = model_config.memory_footprint()\n",
    "print(f\"\\nEstimated training memory: {memory['total_training_gb']:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Update vocab size in config\n",
    "model_config.vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ReasoningTransformer(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use mixed precision\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "print(f\"Model on: {device}\")\n",
    "print(f\"Training dtype: {dtype}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Data Loading\n",
    "\n",
    "We use streaming datasets to avoid downloading everything upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class StreamingTextDataset(IterableDataset):\n    \"\"\"\n    Streams text from multiple datasets with weighted sampling.\n    Handles tokenization and chunking on-the-fly.\n    \"\"\"\n    \n    def __init__(self, dataset_configs, tokenizer, max_length=1024, seed=42):\n        self.dataset_configs = dataset_configs\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.seed = seed\n        \n    def __iter__(self):\n        # Load streaming datasets\n        streams = []\n        weights = []\n        \n        for config in self.dataset_configs:\n            try:\n                if config[\"subset\"]:\n                    ds = load_dataset(\n                        config[\"name\"], \n                        config[\"subset\"], \n                        split=\"train\", \n                        streaming=True,\n                    )\n                else:\n                    ds = load_dataset(\n                        config[\"name\"], \n                        split=\"train\", \n                        streaming=True,\n                    )\n                streams.append(iter(ds))\n                weights.append(config[\"weight\"])\n                print(f\"Loaded: {config['name']}\")\n            except Exception as e:\n                print(f\"Warning: Could not load {config['name']}: {e}\")\n        \n        if not streams:\n            raise ValueError(\"No datasets could be loaded!\")\n        \n        # Normalize weights\n        total = sum(weights)\n        weights = [w / total for w in weights]\n        \n        # Buffer for accumulating tokens\n        token_buffer = []\n        \n        import random\n        rng = random.Random(self.seed)\n        \n        while True:\n            # Sample a dataset based on weights\n            idx = rng.choices(range(len(streams)), weights=weights)[0]\n            \n            try:\n                example = next(streams[idx])\n            except StopIteration:\n                # Dataset exhausted, remove it\n                streams.pop(idx)\n                weights.pop(idx)\n                if not streams:\n                    break\n                total = sum(weights)\n                weights = [w / total for w in weights]\n                continue\n            \n            # Get text from example\n            text = self._extract_text(example)\n            if not text:\n                continue\n            \n            # Tokenize\n            tokens = self.tokenizer.encode(text, add_special_tokens=False)\n            token_buffer.extend(tokens)\n            token_buffer.append(self.tokenizer.eos_token_id)\n            \n            # Yield chunks when buffer is full\n            while len(token_buffer) >= self.max_length:\n                chunk = token_buffer[:self.max_length]\n                token_buffer = token_buffer[self.max_length:]\n                \n                yield {\n                    \"input_ids\": torch.tensor(chunk, dtype=torch.long),\n                    \"labels\": torch.tensor(chunk, dtype=torch.long),\n                }\n    \n    def _extract_text(self, example):\n        \"\"\"Extract text from different dataset formats.\"\"\"\n        # Try common field names\n        for field in [\"text\", \"content\", \"article\", \"passage\"]:\n            if field in example and example[field]:\n                return example[field]\n        return None\n\n\ndef collate_fn(batch):\n    \"\"\"Collate batch of examples.\"\"\"\n    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n    labels = torch.stack([x[\"labels\"] for x in batch])\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n\nprint(\"Data loading utilities defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = StreamingTextDataset(\n",
    "    dataset_configs=CONFIG[\"datasets\"],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=CONFIG[\"max_seq_length\"],\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with batch_size={CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    num_training_steps=CONFIG[\"max_steps\"],\n",
    ")\n",
    "\n",
    "# Gradient scaler for mixed precision\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(dtype == torch.float16))\n",
    "\n",
    "print(\"Optimizer and scheduler configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "if CONFIG[\"use_wandb\"]:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=CONFIG[\"wandb_project\"],\n",
    "        name=CONFIG[\"experiment_name\"],\n",
    "        config={\n",
    "            **CONFIG,\n",
    "            \"model_params\": model_config.num_parameters(),\n",
    "            \"model_config\": model_config.to_dict(),\n",
    "        }\n",
    "    )\n",
    "    print(f\"WandB initialized: {CONFIG['experiment_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, step, loss, path):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        \"step\": step,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"config\": model_config.to_dict(),\n",
    "        \"training_config\": CONFIG,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, scheduler):\n",
    "    \"\"\"Load training checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    return checkpoint[\"step\"], checkpoint[\"loss\"]\n",
    "\n",
    "\n",
    "print(\"Checkpoint utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Resume from checkpoint if specified\nstart_step = 0\n\nif CONFIG[\"resume_from\"]:\n    print(f\"Loading checkpoint: {CONFIG['resume_from']}\")\n    checkpoint = torch.load(CONFIG[\"resume_from\"], map_location=device)\n    \n    # Load model weights\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    \n    # Get previous step count\n    prev_steps = checkpoint.get(\"training_steps\", checkpoint.get(\"step\", 0))\n    start_step = prev_steps\n    \n    # Update max_steps to be previous + additional\n    CONFIG[\"max_steps\"] = prev_steps + ADDITIONAL_STEPS\n    \n    print(f\"  Previous training: {prev_steps:,} steps\")\n    print(f\"  This run will add: {ADDITIONAL_STEPS:,} steps\")\n    print(f\"  Target total: {CONFIG['max_steps']:,} steps\")\n    \n    # Reinitialize optimizer and scheduler for the new run\n    # (fresh optimizer often works better than loading old state for continued training)\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=CONFIG[\"learning_rate\"],\n        weight_decay=CONFIG[\"weight_decay\"],\n        betas=(0.9, 0.95),\n    )\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=CONFIG[\"warmup_steps\"],\n        num_training_steps=ADDITIONAL_STEPS,  # Schedule for this run's steps\n    )\n    print(f\"  Fresh optimizer initialized\")\nelse:\n    print(\"Starting fresh training\")\n    CONFIG[\"max_steps\"] = ADDITIONAL_STEPS"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef evaluate(model, tokenizer, num_samples=3):\n    \"\"\"Quick evaluation - generate samples.\"\"\"\n    torch.cuda.empty_cache()\n    \n    try:\n        model.eval()\n        \n        prompts = [\n            \"The capital of France is\",\n            \"In 1969, humans first\",\n            \"Water boils at\",\n        ]\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"Sample generations:\")\n        print(\"=\"*60)\n        \n        for prompt in prompts[:num_samples]:\n            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n            \n            try:\n                output = model.generate(\n                    input_ids,\n                    max_new_tokens=30,\n                    temperature=0.7,\n                    do_sample=True,\n                    top_p=0.9,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n                \n                generated = tokenizer.decode(output[0], skip_special_tokens=True)\n                print(f\"\\nPrompt: {prompt}\")\n                print(f\"Output: {generated}\")\n            except Exception as e:\n                print(f\"\\nPrompt: {prompt}\")\n                print(f\"Generation failed: {type(e).__name__}\")\n        \n        print(\"=\"*60 + \"\\n\")\n    except Exception as e:\n        print(f\"\\nEvaluation skipped: {type(e).__name__}\\n\")\n    finally:\n        model.train()\n        torch.cuda.empty_cache()\n\n\nprint(\"Evaluation function defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MAIN TRAINING LOOP\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING LANGUAGE MODEL PRETRAINING\")\nprint(\"=\"*60)\nprint(f\"Model: {model_config.name}\")\nprint(f\"Parameters: {model_config.num_parameters() / 1e6:.0f}M\")\nprint(f\"Max steps: {CONFIG['max_steps']:,}\")\nprint(f\"Checkpoint dir: {CONFIG['checkpoint_dir']}\")\nprint(\"=\"*60 + \"\\n\")\n\nmodel.train()\nstep = start_step\ntotal_loss = 0\nlog_interval = 100\n\nprogress = tqdm(total=CONFIG[\"max_steps\"], initial=start_step, desc=\"Training\")\n\ntry:\n    for batch in dataloader:\n        if step >= CONFIG[\"max_steps\"]:\n            break\n        \n        # Move to device\n        input_ids = batch[\"input_ids\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        # Forward pass with mixed precision\n        with torch.amp.autocast(\"cuda\", dtype=dtype):\n            outputs = model(input_ids, labels=labels)\n            loss = outputs[\"loss\"] / CONFIG[\"gradient_accumulation\"]\n        \n        # Backward pass\n        scaler.scale(loss).backward()\n        total_loss += loss.item() * CONFIG[\"gradient_accumulation\"]\n        \n        # Gradient accumulation\n        if (step + 1) % CONFIG[\"gradient_accumulation\"] == 0:\n            # Gradient clipping\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"max_grad_norm\"])\n            \n            # Optimizer step\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        step += 1\n        progress.update(1)\n        \n        # Logging\n        if step % log_interval == 0:\n            avg_loss = total_loss / log_interval\n            lr = scheduler.get_last_lr()[0]\n            \n            progress.set_postfix({\n                \"loss\": f\"{avg_loss:.4f}\",\n                \"lr\": f\"{lr:.2e}\",\n                \"ppl\": f\"{math.exp(min(avg_loss, 20)):.1f}\"\n            })\n            \n            if CONFIG[\"use_wandb\"]:\n                wandb.log({\n                    \"train/loss\": avg_loss,\n                    \"train/perplexity\": math.exp(min(avg_loss, 20)),\n                    \"train/learning_rate\": lr,\n                    \"train/step\": step,\n                })\n            \n            total_loss = 0\n        \n        # Checkpoint\n        if step % CONFIG[\"save_every\"] == 0:\n            checkpoint_path = os.path.join(\n                CONFIG[\"checkpoint_dir\"],\n                f\"checkpoint-{step}.pt\"\n            )\n            save_checkpoint(model, optimizer, scheduler, step, avg_loss, checkpoint_path)\n\nexcept KeyboardInterrupt:\n    print(\"\\nTraining interrupted. Saving checkpoint...\")\n    checkpoint_path = os.path.join(CONFIG[\"checkpoint_dir\"], f\"checkpoint-{step}-interrupted.pt\")\n    save_checkpoint(model, optimizer, scheduler, step, total_loss / log_interval if total_loss > 0 else 0, checkpoint_path)\n\nprogress.close()\nprint(f\"\\nTraining completed at step {step}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_path = os.path.join(CONFIG[\"checkpoint_dir\"], \"final-language-model.pt\")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": model_config.to_dict(),\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"training_steps\": step,\n",
    "    \"training_config\": CONFIG,\n",
    "}, final_path)\n",
    "\n",
    "print(f\"\\nFinal model saved to: {final_path}\")\n",
    "print(\"\\nThis checkpoint will be used as the base for reasoning fine-tuning.\")\n",
    "print(\"Next step: Run train_reasoning_specialist.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "evaluate(model, tokenizer, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if CONFIG[\"use_wandb\"]:\n",
    "    wandb.finish()\n",
    "    print(\"WandB run finished.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LANGUAGE MODEL PRETRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final checkpoint: {final_path}\")\n",
    "print(f\"Total steps: {step:,}\")\n",
    "print(\"\\nNext: Use this model as the base for reasoning fine-tuning.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resume Training (if needed)\n",
    "\n",
    "If Colab disconnects, update `CONFIG[\"resume_from\"]` and re-run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available checkpoints\n",
    "import os\n",
    "\n",
    "checkpoint_dir = CONFIG[\"checkpoint_dir\"]\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".pt\")])\n",
    "    print(\"Available checkpoints:\")\n",
    "    for cp in checkpoints:\n",
    "        path = os.path.join(checkpoint_dir, cp)\n",
    "        size_mb = os.path.getsize(path) / 1e6\n",
    "        print(f\"  {cp} ({size_mb:.0f} MB)\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### What this notebook does:\n",
    "1. Trains a 500M parameter language model from scratch\n",
    "2. Uses diverse text data (OpenWebText, Wikipedia, BookCorpus, C4)\n",
    "3. Produces a base model that understands language patterns\n",
    "\n",
    "### What's next:\n",
    "1. Run `train_reasoning_specialist.ipynb` to fine-tune for math/reasoning\n",
    "2. The reasoning specialist will load this language model checkpoint\n",
    "3. Fine-tuning adds reasoning capability on top of language understanding\n",
    "\n",
    "### Tips:\n",
    "- Monitor perplexity (PPL) - should decrease steadily\n",
    "- Good language models reach PPL ~20-30 on diverse text\n",
    "- Sample generations should become more coherent over time\n",
    "- Save checkpoints frequently - Colab can disconnect!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}