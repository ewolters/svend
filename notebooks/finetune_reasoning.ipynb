{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svend Fine-Tuning: Reasoning + Tool Use\n",
    "\n",
    "Fine-tune the base language model on synthetic reasoning data.\n",
    "\n",
    "**Inputs:**\n",
    "- Base checkpoint: `svend-base-200k.pt` (from Drive)\n",
    "- Training data: `conversations.jsonl` + `tool_traces.jsonl`\n",
    "\n",
    "**Output:**\n",
    "- Fine-tuned checkpoint: `svend-reasoning-v1.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive and setup\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q transformers torch accelerate\n",
    "\n",
    "import os\n",
    "os.chdir('/content')\n",
    "!git clone https://github.com/ewolters/svend.git 2>/dev/null || (cd svend && git pull)\n",
    "os.chdir('/content/svend')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/svend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "DRIVE_PATH = '/content/drive/MyDrive/svend'\n",
    "BASE_CHECKPOINT = f'{DRIVE_PATH}/svend-base-200k.pt'  # Your 200K checkpoint\n",
    "CONVERSATIONS_FILE = f'{DRIVE_PATH}/data/conversations.jsonl'\n",
    "TOOL_TRACES_FILE = f'{DRIVE_PATH}/data/tool_traces.jsonl'\n",
    "OUTPUT_CHECKPOINT = f'{DRIVE_PATH}/svend-reasoning-v1.pt'\n",
    "\n",
    "# Training params\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM = 8  # Effective batch = 32\n",
    "LR = 2e-5\n",
    "MAX_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "from src.models.transformer import ReasoningTransformer\n",
    "from src.models.config import TransformerConfig\n",
    "\n",
    "print(f'Loading checkpoint from {BASE_CHECKPOINT}...')\n",
    "checkpoint = torch.load(BASE_CHECKPOINT, map_location='cpu')\n",
    "\n",
    "config_dict = checkpoint.get('config', {})\n",
    "if isinstance(config_dict, dict):\n",
    "    # Remove computed fields\n",
    "    config_dict = {k: v for k, v in config_dict.items() if k != 'head_dim'}\n",
    "    config = TransformerConfig(**config_dict)\n",
    "else:\n",
    "    config = config_dict\n",
    "\n",
    "model = ReasoningTransformer(config)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "# Count params\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model loaded: {params/1e6:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f'Tokenizer vocab size: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class ReasoningDataset(Dataset):\n",
    "    def __init__(self, conversations_file, tool_traces_file, tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        \n",
    "        # Load conversations (prompt/response format)\n",
    "        if Path(conversations_file).exists():\n",
    "            with open(conversations_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    text = f\"Q: {data['prompt']}\\nA: {data['response']}\"\n",
    "                    self.examples.append(text)\n",
    "            print(f'Loaded {len(self.examples)} conversations')\n",
    "        \n",
    "        # Load tool traces (reasoning format)\n",
    "        n_conv = len(self.examples)\n",
    "        if Path(tool_traces_file).exists():\n",
    "            with open(tool_traces_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    # Format reasoning trace\n",
    "                    text = f\"Q: {data['question']}\\n\"\n",
    "                    for step in data.get('reasoning', []):\n",
    "                        text += f\"Step {step.get('step', '?')}: {step.get('content', '')}\\n\"\n",
    "                        if 'tool_call' in step:\n",
    "                            tc = step['tool_call']\n",
    "                            text += f\"<|tool_call|>{tc.get('name', '')}({json.dumps(tc.get('args', {}))})<|/tool_call|>\\n\"\n",
    "                        if 'tool_result' in step:\n",
    "                            text += f\"<|tool_result|>{step['tool_result']}<|/tool_result|>\\n\"\n",
    "                    text += f\"A: {data.get('answer', '')}\"\n",
    "                    self.examples.append(text)\n",
    "            print(f'Loaded {len(self.examples) - n_conv} tool traces')\n",
    "        \n",
    "        print(f'Total examples: {len(self.examples)}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoded['input_ids'].squeeze()\n",
    "        attention_mask = encoded['attention_mask'].squeeze()\n",
    "        \n",
    "        # Labels = input_ids (causal LM)\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100  # Ignore padding\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = ReasoningDataset(\n",
    "    CONVERSATIONS_FILE,\n",
    "    TOOL_TRACES_FILE,\n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(dataloader) * EPOCHS // GRAD_ACCUM\n",
    "warmup_steps = total_steps // 10\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LR,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1\n",
    ")\n",
    "\n",
    "print(f'Total steps: {total_steps}')\n",
    "print(f'Warmup steps: {warmup_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "scaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None\n",
    "\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    for step, batch in enumerate(pbar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if scaler:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss / GRAD_ACCUM\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / GRAD_ACCUM\n",
    "            loss.backward()\n",
    "        \n",
    "        epoch_loss += loss.item() * GRAD_ACCUM\n",
    "        \n",
    "        # Gradient accumulation step\n",
    "        if (step + 1) % GRAD_ACCUM == 0:\n",
    "            if scaler:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item() * GRAD_ACCUM:.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Save best\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        print(f'New best! Saving checkpoint...')\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config_dict,\n",
    "            'tokenizer_name': 'gpt2',\n",
    "            'training_steps': global_step,\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'fine_tuned_on': ['conversations', 'tool_traces']\n",
    "        }, OUTPUT_CHECKPOINT)\n",
    "        print(f'Saved to {OUTPUT_CHECKPOINT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final save\n",
    "print('\\n' + '='*60)\n",
    "print('TRAINING COMPLETE')\n",
    "print('='*60)\n",
    "print(f'Final loss: {best_loss:.4f}')\n",
    "print(f'Checkpoint saved to: {OUTPUT_CHECKPOINT}')\n",
    "print('\\nDownload and test locally with:')\n",
    "print('  py -3 scripts/quick_test.py checkpoints/svend-reasoning-v1.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
