{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svend Fine-Tuning: Reasoning + Tool Use\n",
    "\n",
    "Fine-tune the base language model on synthetic reasoning data.\n",
    "\n",
    "**Setup:** \n",
    "1. Add `ANTHROPIC_API_KEY` to Colab Secrets (key icon in left sidebar)\n",
    "2. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive, setup, and load API key from Colab Secrets\n",
    "from google.colab import drive, userdata\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "try:\n",
    "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "    os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
    "    print('Loaded ANTHROPIC_API_KEY from Colab Secrets')\n",
    "except Exception as e:\n",
    "    ANTHROPIC_API_KEY = None\n",
    "    print(f'Could not load from Secrets: {e}')\n",
    "    print('Add ANTHROPIC_API_KEY to Colab Secrets (key icon in left sidebar)')\n",
    "\n",
    "!pip install -q transformers accelerate anthropic\n",
    "\n",
    "os.chdir('/content')\n",
    "!git clone https://github.com/ewolters/svend.git 2>/dev/null || (cd svend && git pull)\n",
    "os.chdir('/content/svend')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/svend')\n",
    "\n",
    "print('\\n=== Available checkpoints ===')\n",
    "!ls -lh /content/drive/MyDrive/svend-checkpoints/language-model/*.pt 2>/dev/null | tail -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Config { display-mode: \"form\" }\n",
    "#@markdown ### Paths\n",
    "DRIVE_BASE = '/content/drive/MyDrive/svend-checkpoints'  #@param {type:\"string\"}\n",
    "BASE_CHECKPOINT = f'{DRIVE_BASE}/language-model/checkpoint-200000.pt'  #@param {type:\"string\"}\n",
    "OUTPUT_NAME = 'svend-reasoning-v1.pt'  #@param {type:\"string\"}\n",
    "OUTPUT_CHECKPOINT = f'{DRIVE_BASE}/language-model/{OUTPUT_NAME}'\n",
    "\n",
    "CONVERSATIONS_FILE = '/content/svend/data/conversations.jsonl'\n",
    "TOOL_TRACES_FILE = '/content/svend/data/tool_traces.jsonl'\n",
    "\n",
    "#@markdown ### Training params\n",
    "EPOCHS = 3  #@param {type:\"integer\"}\n",
    "BATCH_SIZE = 4  #@param {type:\"integer\"}\n",
    "GRAD_ACCUM = 8  #@param {type:\"integer\"}\n",
    "LR = 2e-5  #@param {type:\"number\"}\n",
    "MAX_LENGTH = 1024  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Data generation (set to 0 to skip)\n",
    "EXPAND_SEEDS_PER = 50  #@param {type:\"integer\"}\n",
    "TOOL_TRACES_COUNT = 500  #@param {type:\"integer\"}\n",
    "\n",
    "from pathlib import Path\n",
    "print('Checking checkpoint...')\n",
    "print(f'  Base checkpoint: {\"OK\" if Path(BASE_CHECKPOINT).exists() else \"NOT FOUND\"} - {BASE_CHECKPOINT}')\n",
    "print(f'  Output:          {OUTPUT_CHECKPOINT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option 1: Use seed files directly (no API needed, ~80 examples)\n# Option 2: Generate more data with API (set EXPAND_SEEDS_PER > 0 above)\n\nimport json\nfrom pathlib import Path\n\n!mkdir -p /content/svend/data\n\n# Load seeds directly as training data\nseed_dir = Path('/content/svend/data/seeds')\nexamples = []\n\nif seed_dir.exists():\n    for seed_file in seed_dir.glob('*.jsonl'):\n        with open(seed_file, 'r') as f:\n            for line in f:\n                if line.strip():\n                    examples.append(json.loads(line))\n    print(f'Loaded {len(examples)} seed examples directly')\n    \n    # Write to conversations file\n    with open(CONVERSATIONS_FILE, 'w') as f:\n        for ex in examples:\n            f.write(json.dumps(ex) + '\\n')\n    print(f'Wrote to {CONVERSATIONS_FILE}')\nelse:\n    print(f'ERROR: Seeds not found at {seed_dir}')\n    print('Run: cd /content && rm -rf svend && git clone https://github.com/ewolters/svend.git')\n\n# Skip tool traces for now - train on seeds only\nprint(f'\\nTraining will use {len(examples)} examples')\nprint('(For more data, run expand_seeds.py locally with your API key)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "from src.models.transformer import ReasoningTransformer\n",
    "from src.models.config import TransformerConfig\n",
    "\n",
    "print(f'Loading checkpoint from {BASE_CHECKPOINT}...')\n",
    "checkpoint = torch.load(BASE_CHECKPOINT, map_location='cpu', weights_only=False)\n",
    "\n",
    "config_dict = checkpoint.get('config', {})\n",
    "if isinstance(config_dict, dict):\n",
    "    config_dict = {k: v for k, v in config_dict.items() if k != 'head_dim'}\n",
    "    config = TransformerConfig(**config_dict)\n",
    "else:\n",
    "    config = config_dict\n",
    "\n",
    "model = ReasoningTransformer(config)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model loaded: {params/1e6:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f'Tokenizer vocab size: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningDataset(Dataset):\n",
    "    def __init__(self, conversations_file, tool_traces_file, tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        \n",
    "        if Path(conversations_file).exists():\n",
    "            with open(conversations_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    self.examples.append(f\"Q: {data['prompt']}\\nA: {data['response']}\")\n",
    "            print(f'Loaded {len(self.examples)} conversations')\n",
    "        \n",
    "        n_conv = len(self.examples)\n",
    "        if Path(tool_traces_file).exists():\n",
    "            with open(tool_traces_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    text = f\"Q: {data['question']}\\n\"\n",
    "                    for step in data.get('reasoning', []):\n",
    "                        text += f\"Step {step.get('step', '?')}: {step.get('content', '')}\\n\"\n",
    "                        if 'tool_call' in step:\n",
    "                            tc = step['tool_call']\n",
    "                            text += f\"<|tool_call|>{tc.get('name', '')}({json.dumps(tc.get('args', {}))})<|/tool_call|>\\n\"\n",
    "                        if 'tool_result' in step:\n",
    "                            text += f\"<|tool_result|>{step['tool_result']}<|/tool_result|>\\n\"\n",
    "                    text += f\"A: {data.get('answer', '')}\"\n",
    "                    self.examples.append(text)\n",
    "            print(f'Loaded {len(self.examples) - n_conv} tool traces')\n",
    "        \n",
    "        print(f'Total examples: {len(self.examples)}')\n",
    "        if len(self.examples) == 0:\n",
    "            raise ValueError('No training data found!')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.tokenizer(self.examples[idx], truncation=True, max_length=self.max_length, padding='max_length', return_tensors='pt')\n",
    "        input_ids = encoded['input_ids'].squeeze()\n",
    "        attention_mask = encoded['attention_mask'].squeeze()\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReasoningDataset(CONVERSATIONS_FILE, TOOL_TRACES_FILE, tokenizer, max_length=MAX_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "total_steps = max(1, len(dataloader) * EPOCHS // GRAD_ACCUM)\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LR, total_steps=total_steps, pct_start=0.1)\n",
    "\n",
    "print(f'Training examples: {len(dataset)}')\n",
    "print(f'Batches per epoch: {len(dataloader)}')\n",
    "print(f'Total steps: {total_steps}')\n",
    "print(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model.train()\nscaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None\nglobal_step = 0\nbest_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    epoch_loss = 0\n    optimizer.zero_grad()\n    \n    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n    for step, batch in enumerate(pbar):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        if scaler:\n            with torch.amp.autocast('cuda'):\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs['loss'] / GRAD_ACCUM  # dict access, not attribute\n            scaler.scale(loss).backward()\n        else:\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs['loss'] / GRAD_ACCUM\n            loss.backward()\n        \n        epoch_loss += loss.item() * GRAD_ACCUM\n        \n        if (step + 1) % GRAD_ACCUM == 0:\n            if scaler:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            global_step += 1\n        \n        pbar.set_postfix({'loss': f'{loss.item() * GRAD_ACCUM:.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n    \n    avg_loss = epoch_loss / len(dataloader)\n    print(f'Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}')\n    \n    if avg_loss < best_loss:\n        best_loss = avg_loss\n        print(f'Saving checkpoint...')\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'config': config_dict,\n            'tokenizer_name': 'gpt2',\n            'training_steps': global_step,\n            'epoch': epoch + 1,\n            'loss': avg_loss,\n            'fine_tuned_on': ['conversations', 'tool_traces']\n        }, OUTPUT_CHECKPOINT)\n        print(f'Saved to {OUTPUT_CHECKPOINT}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('TRAINING COMPLETE')\n",
    "print('='*60)\n",
    "print(f'Final loss: {best_loss:.4f}')\n",
    "print(f'Checkpoint: {OUTPUT_CHECKPOINT}')\n",
    "print('\\nTest locally: py -3 scripts/quick_test.py checkpoints/svend-reasoning-v1.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}