{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Base Reasoner Model\n",
        "\n",
        "This notebook trains **just the base reasoning model** - no router, no ensemble.\n",
        "\n",
        "**Requirements:** Colab Pro+ with A100\n",
        "\n",
        "**IMPORTANT:** Run cells in order. If you re-clone the repo, restart the runtime (Runtime -> Restart runtime) before continuing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Drive for checkpoints\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo (fresh each time to get latest)\n",
        "!rm -rf /content/svend\n",
        "!git clone https://github.com/ewolters/svend.git /content/svend\n",
        "%cd /content/svend\n",
        "\n",
        "# Show commit to verify we have latest\n",
        "!git log -1 --oneline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch transformers datasets accelerate wandb\n",
        "!pip install -q sympy sentencepiece tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTANT: Fresh imports - clear any cached modules\n",
        "import sys\n",
        "\n",
        "# Remove any cached svend/src modules\n",
        "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('src')]\n",
        "for mod in modules_to_remove:\n",
        "    del sys.modules[mod]\n",
        "\n",
        "# Add to path\n",
        "if '/content/svend' not in sys.path:\n",
        "    sys.path.insert(0, '/content/svend')\n",
        "\n",
        "# Now import\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"bf16: {torch.cuda.is_bf16_supported()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify model imports work and labels parameter exists\n",
        "from src.models.config import get_config\n",
        "from src.models.transformer import ReasoningTransformer\n",
        "import inspect\n",
        "\n",
        "# Check that forward() accepts labels\n",
        "sig = inspect.signature(ReasoningTransformer.forward)\n",
        "params = list(sig.parameters.keys())\n",
        "print(f\"ReasoningTransformer.forward() parameters: {params}\")\n",
        "\n",
        "if 'labels' in params:\n",
        "    print(\"\\n[OK] 'labels' parameter found - imports are fresh\")\n",
        "else:\n",
        "    print(\"\\n[ERROR] 'labels' parameter NOT found!\")\n",
        "    print(\"Please restart runtime: Runtime -> Restart runtime\")\n",
        "    print(\"Then re-run all cells from the beginning.\")\n",
        "    raise RuntimeError(\"Stale imports detected - restart runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training config\n",
        "CONFIG = {\n",
        "    \"model_size\": \"125m\",  # Start small: 125m, 350m, 500m, 1b\n",
        "    \"epochs\": 3,\n",
        "    \"batch_size\": 8,\n",
        "    \"gradient_accumulation\": 4,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"max_length\": 512,\n",
        "    \"warmup_steps\": 100,\n",
        "    \n",
        "    # Checkpointing\n",
        "    \"save_steps\": 500,\n",
        "    \"checkpoint_dir\": \"/content/drive/MyDrive/svend-checkpoints/base-reasoner\",\n",
        "    \n",
        "    # WandB\n",
        "    \"use_wandb\": True,\n",
        "    \"wandb_project\": \"svend\",\n",
        "    \"run_name\": \"base-reasoner-125m\",\n",
        "}\n",
        "\n",
        "# Create checkpoint dir\n",
        "import os\n",
        "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
        "\n",
        "print(\"Config:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WandB login (optional - set use_wandb to False to skip)\n",
        "if CONFIG[\"use_wandb\"]:\n",
        "    import wandb\n",
        "    wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Load reasoning datasets\n",
        "datasets_to_load = []\n",
        "\n",
        "# GSM8K - math word problems\n",
        "try:\n",
        "    gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "    print(f\"GSM8K: {len(gsm8k)} examples\")\n",
        "    datasets_to_load.append((\"gsm8k\", gsm8k))\n",
        "except Exception as e:\n",
        "    print(f\"GSM8K failed: {e}\")\n",
        "\n",
        "# MATH - harder math problems  \n",
        "try:\n",
        "    math_ds = load_dataset(\"lighteval/MATH\", split=\"train\", trust_remote_code=True)\n",
        "    print(f\"MATH: {len(math_ds)} examples\")\n",
        "    datasets_to_load.append((\"math\", math_ds))\n",
        "except Exception as e:\n",
        "    print(f\"MATH failed: {e}\")\n",
        "\n",
        "print(f\"\\nLoaded {len(datasets_to_load)} datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use GPT-2 tokenizer as base\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def format_gsm8k(example):\n",
        "    \"\"\"Format GSM8K for training.\"\"\"\n",
        "    return {\n",
        "        \"text\": f\"Question: {example['question']}\\n\\nAnswer: {example['answer']}\"\n",
        "    }\n",
        "\n",
        "def format_math(example):\n",
        "    \"\"\"Format MATH dataset for training.\"\"\"\n",
        "    return {\n",
        "        \"text\": f\"Problem: {example['problem']}\\n\\nSolution: {example['solution']}\"\n",
        "    }\n",
        "\n",
        "# Format datasets\n",
        "formatted = []\n",
        "for name, ds in datasets_to_load:\n",
        "    if name == \"gsm8k\":\n",
        "        formatted.append(ds.map(format_gsm8k, remove_columns=ds.column_names))\n",
        "    elif name == \"math\":\n",
        "        formatted.append(ds.map(format_math, remove_columns=ds.column_names))\n",
        "\n",
        "# Combine\n",
        "if formatted:\n",
        "    train_dataset = concatenate_datasets(formatted)\n",
        "    print(f\"Combined dataset: {len(train_dataset)} examples\")\n",
        "else:\n",
        "    raise ValueError(\"No datasets loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize\n",
        "def tokenize(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=CONFIG[\"max_length\"],\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing...\")\n",
        "tokenized = train_dataset.map(\n",
        "    tokenize,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"],\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "tokenized.set_format(\"torch\")\n",
        "\n",
        "print(f\"Tokenized: {len(tokenized)} examples\")\n",
        "print(f\"Sample keys: {list(tokenized[0].keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get model config\n",
        "model_config = get_config(CONFIG[\"model_size\"])\n",
        "model_config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "print(f\"Model config:\")\n",
        "print(f\"  Hidden size: {model_config.hidden_size}\")\n",
        "print(f\"  Layers: {model_config.num_hidden_layers}\")\n",
        "print(f\"  Heads: {model_config.num_attention_heads}\")\n",
        "print(f\"  Vocab size: {model_config.vocab_size}\")\n",
        "\n",
        "# Create model\n",
        "model = ReasoningTransformer(model_config)\n",
        "model = model.cuda()\n",
        "\n",
        "# Count parameters\n",
        "params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nParameters: {params:,} ({params/1e6:.1f}M)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick sanity check - verify forward pass with labels works\n",
        "print(\"Testing forward pass with labels...\")\n",
        "\n",
        "test_input = torch.randint(0, model_config.vocab_size, (2, 64)).cuda()\n",
        "test_mask = torch.ones_like(test_input).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=test_input,\n",
        "        attention_mask=test_mask,\n",
        "        labels=test_input\n",
        "    )\n",
        "\n",
        "print(f\"  Output keys: {list(outputs.keys())}\")\n",
        "print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
        "print(f\"  Logits shape: {outputs['logits'].shape}\")\n",
        "print(\"\\n[OK] Forward pass with labels works!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(\n",
        "    tokenized,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=0.01)\n",
        "\n",
        "# Scheduler\n",
        "total_steps = len(train_loader) * CONFIG[\"epochs\"] // CONFIG[\"gradient_accumulation\"]\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "print(f\"Training setup:\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Epochs: {CONFIG['epochs']}\")\n",
        "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"  Gradient accumulation: {CONFIG['gradient_accumulation']}\")\n",
        "print(f\"  Effective batch: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WandB init\n",
        "if CONFIG[\"use_wandb\"]:\n",
        "    import wandb\n",
        "    wandb.init(\n",
        "        project=CONFIG[\"wandb_project\"],\n",
        "        name=CONFIG[\"run_name\"],\n",
        "        config=CONFIG\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "global_step = 0\n",
        "accumulation_step = 0\n",
        "\n",
        "# Mixed precision\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "use_bf16 = torch.cuda.is_bf16_supported()\n",
        "dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "\n",
        "print(f\"\\nStarting training...\")\n",
        "print(f\"Mixed precision: {dtype}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "    \n",
        "    for batch in pbar:\n",
        "        input_ids = batch[\"input_ids\"].cuda()\n",
        "        attention_mask = batch[\"attention_mask\"].cuda()\n",
        "        \n",
        "        # Forward with mixed precision\n",
        "        with torch.amp.autocast('cuda', dtype=dtype):\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=input_ids  # Causal LM: predict next token\n",
        "            )\n",
        "            loss = outputs[\"loss\"] / CONFIG[\"gradient_accumulation\"]\n",
        "        \n",
        "        # Backward\n",
        "        scaler.scale(loss).backward()\n",
        "        \n",
        "        accumulation_step += 1\n",
        "        epoch_loss += loss.item() * CONFIG[\"gradient_accumulation\"]\n",
        "        num_batches += 1\n",
        "        \n",
        "        # Optimizer step\n",
        "        if accumulation_step >= CONFIG[\"gradient_accumulation\"]:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "            \n",
        "            global_step += 1\n",
        "            accumulation_step = 0\n",
        "            \n",
        "            # Logging\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"step\": global_step})\n",
        "            \n",
        "            if CONFIG[\"use_wandb\"]:\n",
        "                wandb.log({\n",
        "                    \"loss\": avg_loss,\n",
        "                    \"lr\": scheduler.get_last_lr()[0],\n",
        "                    \"step\": global_step,\n",
        "                    \"epoch\": epoch\n",
        "                })\n",
        "            \n",
        "            # Save checkpoint\n",
        "            if global_step % CONFIG[\"save_steps\"] == 0:\n",
        "                ckpt_path = f\"{CONFIG['checkpoint_dir']}/step_{global_step:06d}.pt\"\n",
        "                torch.save({\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "                    \"global_step\": global_step,\n",
        "                    \"epoch\": epoch,\n",
        "                    \"config\": CONFIG,\n",
        "                }, ckpt_path)\n",
        "                print(f\"\\nSaved checkpoint: {ckpt_path}\")\n",
        "    \n",
        "    print(f\"Epoch {epoch+1} complete. Avg loss: {epoch_loss/num_batches:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_path = f\"{CONFIG['checkpoint_dir']}/final.pt\"\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"config\": CONFIG,\n",
        "    \"model_config\": model_config.__dict__ if hasattr(model_config, '__dict__') else str(model_config),\n",
        "}, final_path)\n",
        "print(f\"Saved final model: {final_path}\")\n",
        "\n",
        "if CONFIG[\"use_wandb\"]:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quick Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test generation\n",
        "model.eval()\n",
        "\n",
        "test_prompt = \"Question: What is 15% of 200?\\n\\nAnswer:\"\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Resume Training (if disconnected)\n",
        "\n",
        "If Colab disconnects:\n",
        "1. Run cells 1-6 (Setup section)\n",
        "2. Run Config and Data cells\n",
        "3. Run Create Model cell\n",
        "4. Then run the cell below with your checkpoint path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume from checkpoint - update this path!\n",
        "RESUME_FROM = \"/content/drive/MyDrive/svend-checkpoints/base-reasoner/step_000500.pt\"\n",
        "\n",
        "checkpoint = torch.load(RESUME_FROM)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "global_step = checkpoint[\"global_step\"]\n",
        "start_epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "print(f\"Resumed from step {global_step}, epoch {start_epoch}\")\n",
        "print(\"Now run the training loop cell to continue.\")"
      ]
    }
  ]
}
