{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Reasoning Model Training on A100\n",
    "\n",
    "This notebook trains a custom 7B parameter reasoning model from scratch,\n",
    "then distills it to a 500M student model for local inference.\n",
    "\n",
    "**Requirements:**\n",
    "- A100 80GB GPU (Colab Pro+)\n",
    "- ~24-48 hours for full 7B training\n",
    "- ~4-8 hours for distillation\n",
    "\n",
    "**Outputs:**\n",
    "- 7B teacher model checkpoint\n",
    "- 500M student model (for HP tower deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if \"A100\" in gpu_name and gpu_memory > 70:\n",
    "        print(\"\\n✓ A100 80GB detected - perfect for 7B training!\")\n",
    "    elif gpu_memory > 40:\n",
    "        print(\"\\n⚠ Large GPU detected - may need gradient checkpointing\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Smaller GPU - consider training 3B model instead\")\n",
    "else:\n",
    "    print(\"❌ No GPU - training will be extremely slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets accelerate wandb\n",
    "\n",
    "# Clone the reasoning-lab repository\n",
    "!git clone https://github.com/YOUR_USERNAME/reasoning-lab.git\n",
    "%cd reasoning-lab\n",
    "\n",
    "# Or mount Google Drive if storing there\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/reasoning-lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "from src.models import create_model, create_7b_config, create_500m_config\n",
    "from src.data import (\n",
    "    create_tokenizer, \n",
    "    DatasetConfig, \n",
    "    create_combined_dataset,\n",
    "    ReasoningDataset,\n",
    "    create_dataloaders,\n",
    ")\n",
    "from src.training import TrainingConfig, Trainer\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings based on your needs and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_size\": \"7b\",  # \"3b\", \"7b\", or \"13b\"\n",
    "    \n",
    "    # Data\n",
    "    \"num_samples\": 100000,  # None for full dataset (~150k)\n",
    "    \"max_seq_length\": 2048,\n",
    "    \n",
    "    # Training\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 2,  # Per GPU\n",
    "    \"gradient_accumulation\": 16,  # Effective batch = 32\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"warmup_steps\": 500,\n",
    "    \n",
    "    # Efficiency\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"bf16\": True,\n",
    "    \n",
    "    # Saving\n",
    "    \"save_steps\": 500,\n",
    "    \"output_dir\": \"checkpoints/teacher_7b\",\n",
    "    \n",
    "    # Logging\n",
    "    \"use_wandb\": True,\n",
    "    \"wandb_project\": \"reasoning-7b\",\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_section"
   },
   "source": [
    "## 1. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "if CONFIG[\"model_size\"] == \"7b\":\n",
    "    model_config = create_7b_config()\n",
    "elif CONFIG[\"model_size\"] == \"3b\":\n",
    "    from src.models import create_3b_config\n",
    "    model_config = create_3b_config()\n",
    "elif CONFIG[\"model_size\"] == \"13b\":\n",
    "    from src.models import create_13b_config\n",
    "    model_config = create_13b_config()\n",
    "\n",
    "model_config.gradient_checkpointing = CONFIG[\"gradient_checkpointing\"]\n",
    "\n",
    "print(f\"Model: {model_config.name}\")\n",
    "print(f\"Parameters: {model_config.num_parameters() / 1e9:.2f}B\")\n",
    "\n",
    "# Memory estimate\n",
    "mem = model_config.memory_footprint(dtype_bytes=2, batch_size=CONFIG[\"batch_size\"])\n",
    "print(f\"Estimated training memory: {mem['total_training_gb']:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_model"
   },
   "outputs": [],
   "source": "# Initialize model\nmodel = create_model(model_config)\nprint(f\"Created model with {model.num_parameters() / 1e9:.2f}B parameters\")\n\n# Load tokenizer (using Mistral - open source, similar vocab to Llama)\ntokenizer = create_tokenizer(\n    base_tokenizer=\"mistralai/Mistral-7B-v0.1\",\n    vocab_size=model_config.vocab_size,\n    add_reasoning_tokens=True,\n)\nprint(f\"Tokenizer vocabulary: {len(tokenizer)} tokens\")\n\n# Resize embeddings if needed\nif len(tokenizer) != model_config.vocab_size:\n    print(f\"Resizing embeddings to match tokenizer...\")\n    model.embed_tokens = torch.nn.Embedding(len(tokenizer), model_config.hidden_size)\n    if model.lm_head is not None:\n        model.lm_head = torch.nn.Linear(model_config.hidden_size, len(tokenizer), bias=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## 2. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Configure data sources\n",
    "data_config = DatasetConfig(\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Limit samples if specified\n",
    "if CONFIG[\"num_samples\"]:\n",
    "    samples_per_source = CONFIG[\"num_samples\"] // len(data_config.sources)\n",
    "    for source in data_config.sources:\n",
    "        source[\"sample_size\"] = min(source.get(\"sample_size\", samples_per_source), samples_per_source)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "dataset = create_combined_dataset(data_config)\n",
    "print(f\"Total examples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "# Train/val split\n",
    "train_size = int(0.98 * len(dataset))\n",
    "train_data = dataset.select(range(train_size))\n",
    "val_data = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = ReasoningDataset(train_data, tokenizer, max_length=CONFIG[\"max_seq_length\"])\n",
    "val_dataset = ReasoningDataset(val_data, tokenizer, max_length=CONFIG[\"max_seq_length\"])\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = create_dataloaders(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(dataloaders['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_section"
   },
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "outputs": [],
   "source": [
    "# Optional: Login to Weights & Biases\n",
    "if CONFIG[\"use_wandb\"]:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "\n",
    "# Training configuration\n",
    "training_config = TrainingConfig(\n",
    "    num_epochs=CONFIG[\"epochs\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    mixed_precision=True,\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    use_wandb=CONFIG[\"use_wandb\"],\n",
    "    wandb_project=CONFIG[\"wandb_project\"],\n",
    ")\n",
    "\n",
    "print(f\"Effective batch size: {training_config.effective_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Create trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    config=training_config,\n",
    "    train_dataloader=dataloaders[\"train\"],\n",
    "    eval_dataloader=dataloaders.get(\"val\"),\n",
    ")\n",
    "\n",
    "# Train!\n",
    "results = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total steps: {results['total_steps']}\")\n",
    "print(f\"Final loss: {results['final_loss']:.4f}\")\n",
    "print(f\"Training time: {results['training_time']/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "distill_section"
   },
   "source": [
    "## 4. Distill to 500M Student\n",
    "\n",
    "Now we distill the trained 7B teacher to a 500M student model\n",
    "that can run on your HP tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_distillation"
   },
   "outputs": [],
   "source": [
    "from src.training import DistillationConfig, DistillationTrainer\n",
    "\n",
    "# Create student model\n",
    "student_config = create_500m_config()\n",
    "student = create_model(student_config)\n",
    "\n",
    "print(f\"Teacher: {model_config.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"Student: {student_config.num_parameters() / 1e6:.0f}M\")\n",
    "print(f\"Compression: {model_config.num_parameters() / student_config.num_parameters():.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "distill"
   },
   "outputs": [],
   "source": [
    "# Distillation configuration\n",
    "distill_config = DistillationConfig(\n",
    "    temperature=2.0,\n",
    "    alpha_ce=0.5,\n",
    "    alpha_kl=0.5,\n",
    "    alpha_hidden=0.1,\n",
    "    freeze_teacher=True,\n",
    ")\n",
    "\n",
    "# Optimizer for student\n",
    "optimizer = torch.optim.AdamW(\n",
    "    student.parameters(),\n",
    "    lr=5e-5,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "\n",
    "# Create distillation trainer\n",
    "distill_trainer = DistillationTrainer(\n",
    "    teacher=model,\n",
    "    student=student,\n",
    "    config=distill_config,\n",
    "    train_dataloader=dataloaders[\"train\"],\n",
    "    eval_dataloader=dataloaders.get(\"val\"),\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "# Distill for 5 epochs\n",
    "print(\"Starting distillation...\")\n",
    "for epoch in range(5):\n",
    "    print(f\"\\nEpoch {epoch + 1}/5\")\n",
    "    losses = distill_trainer.train_epoch()\n",
    "    print(f\"  Losses: {losses}\")\n",
    "    \n",
    "    if dataloaders.get(\"val\"):\n",
    "        eval_metrics = distill_trainer.evaluate()\n",
    "        print(f\"  Eval: {eval_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_student"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save student model\n",
    "student_dir = \"checkpoints/student_500m\"\n",
    "os.makedirs(student_dir, exist_ok=True)\n",
    "\n",
    "torch.save(student.state_dict(), f\"{student_dir}/model.pt\")\n",
    "student_config.save(f\"{student_dir}/config.json\")\n",
    "\n",
    "print(f\"Student model saved to {student_dir}\")\n",
    "print(f\"Model size: {os.path.getsize(f'{student_dir}/model.pt') / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_section"
   },
   "source": [
    "## 5. Test the Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_student"
   },
   "outputs": [],
   "source": [
    "# Test inference\n",
    "student.eval()\n",
    "device = next(student.parameters()).device\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is 15 + 27? Think step by step.\",\n",
    "    \"If all dogs are mammals, and some mammals can fly, can some dogs fly? Explain your reasoning.\",\n",
    "    \"Debug this Python code: def add(a, b): return a - b\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = student.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Response: {response[len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## 6. Download Models\n",
    "\n",
    "Download the trained models to your local machine or Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Option 1: Copy to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp -r checkpoints/student_500m /content/drive/MyDrive/reasoning-models/\n",
    "!cp -r checkpoints/teacher_7b/final /content/drive/MyDrive/reasoning-models/teacher_7b\n",
    "\n",
    "print(\"Models copied to Google Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_direct"
   },
   "outputs": [],
   "source": [
    "# Option 2: Download directly\n",
    "from google.colab import files\n",
    "\n",
    "# Compress student model\n",
    "!tar -czvf student_500m.tar.gz checkpoints/student_500m\n",
    "\n",
    "# Download\n",
    "files.download('student_500m.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Deploy on HP Tower:**\n",
    "   - Copy the 500M model to your HP machine\n",
    "   - See `scripts/inference_local.py` for running locally\n",
    "\n",
    "2. **Raspberry Pi Cluster:**\n",
    "   - The 500M model can be quantized to INT8 for Pi deployment\n",
    "   - See `src/deployment/quantize.py` (coming soon)\n",
    "\n",
    "3. **Further Training:**\n",
    "   - Generate more synthetic data with `src/data/synthetic.py`\n",
    "   - Fine-tune on domain-specific reasoning tasks\n",
    "\n",
    "4. **Evaluation:**\n",
    "   - Run `scripts/evaluate_models.py --compare` to compare teacher/student\n",
    "   - Check retention metrics for distillation quality"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}